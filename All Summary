
自我介绍：
	面试官你好，我叫吕超毅，17年毕业于华中科技大学软件工程学院，现在在shopee video supply团队负责Java开发

	我们的团队职责主要有两块：
	业务层面
		video的创作者相关，包括：创作工具、内容变现、流量激励、任务相关、主播助手
		live相关，面相卖家的内容，包括：直播大屏、直播诊断、AIGC
		内容供给相关，包括：短视频图片的爬取
	技术层面
		数据中台的职责，提供数据服务，包括主播端的实时看板、以及面向运营的一些离线数据的统计和可视化平台

	我在团队中的职责也分两块(核心开发 + 技术方案设计 + 线上保障)：
	业务层面
		日常业务owner项目以及代码开发
			技术选型
			输出技术文档
			资源申请
			风险评估
			技术分享
	技术层面
		项目的维护以及架构优化
			日常指标监控
			大促期间的问题定位
			系统的稳定性优化
			告警监控问题处理

	业务团队介绍
		团队职责
			创作者内容供给 主播流量激励、任务体系、创作周报
			卖家内容供给 直播实时大屏 
			视频图片内容供给 商品图爬取，aigc图片产出
			数据中台 实时数据、离线数据 聚合看板
		数据范围
			直播数据
			视频数据
			商品数据
			创作者数据
		


	之前主要参与过的项目有：直播大屏系统（基本的业务代码， 日常的报警监控，es的优化，分库分表）、特征平台（负责架构设计，项目搭建，风险评估），有道少儿英语的订单系统（主要负责订单系统的重构和优化）

	主要的技术栈有：Java全家桶（包括Spring、SpringBoot之类的），数据库包括：MySQL、Redis、MongoDB等，其他中间件的话：kafka、ElasticSearch用过一些


	业务系统
		creator-centor 创作者入口
			创作者数据处理 core data bin log
			作者推荐服务：给用户推荐作者，帮助作者涨粉
			榜单服务：给用户展示视频排名、作者排名榜单
			任务系统：让创作者完成一系列任务，提高创作者积极性
			课程服务：给创作者提供课程服务，增加专业知识
			带货数据分析：给作者提供视频带货订单分析
			权益流量包：给作者发放流量包，失活作者召回
		creator-market 撮合平台，创作者洽谈合作的平台
			邀约系统：同谷歌日历
			创作者信息维护
			通知提醒服务
		admin-platform
			流量扶持：发布活动，给某些大v的视频增加流量
			保量数据分析：数仓数据聚合筛选同步

	基础服务
		spex4j rpc服务
		config-server 配置中心
		data-park 数据特征标签服务
		creator-payment 支付服务


重点项目介绍：
	
	架构图/项目结构图 todo

	数据流转

	组件相关问题
	


	直播大屏
		背景目标
			卖家？需要在dashboard上查看直播的相关数据指标，包括GMV、like、item sold之类，类似与抖音直播看板
		特点
			能够承载高吞吐量5w wps的直播数据，拥有kv、mysql、es、hbase多数据源存储方案，系统高可用，能够进行亚秒级的近实时直播商品数据的查询，为创作者提供了丰富的多口径多维度的直播以及商品数据展示
		难点
			高并发&低延时
				限流 限流配置项保证核心链路可用
				Kafka	增加分区、压缩、批量发送	扛写入
				消费端	多线程 + 批写 + 状态合并	降 DB 压力
				Redis	Pipeline + Lua + 分片	高并发写入
				DB es 写入	批查批写 分库分表	减缓 IO 压力
			数据稳定性
				 1. 定时对账（异构校验）
				每小时/每天有校验任务对 Redis vs MySQL vs ES 的数据：
				比如：主播 room_id=123，分钟级的 viewer_count 在三个系统里是否一致；
				差异数据打日志或进入补偿队列重新修复；

				典型技术：Flink Batch / Spark / 自研校验脚本；

				2. 数据完整性监控
				每分钟监控 Kafka 消费 TPS 与 Redis 写入成功数、ES 索引数是否匹配；
				超出阈值（比如掉 0 或偏差 >5%）就告警或触发补偿任务；

				3. 关键链路数据持久化 + 回溯补偿
				Kafka topic 数据保留 3-7 天；
				万一 Redis 数据掉了，可以用 Kafka replay 或数据库反推补全；

			冷热数据分层
				热数据 榜单 窗口数据在kv es
				冷数据 直播
			服务可靠性保障
				熔断 下游服务挂了之后，逻辑走旁路，减轻对下游服务的影响
				降级 某些中间件挂掉，优先保证kol主要链路通畅，比如kv挂了，可以从本地缓存取数作为兜底（一致性？）
				限流 上游数据流量大了，可以通过限流器限制消费速度，主要针对非实时的db查询
				全链路压测 流量隔离（打标） 数据隔离（专门的表） 依赖隔离 限流熔断保护机制

		问题
			直播数据按地区拆库之后，id地区仍有大量bin log,2天时间1.2t 原因：单场直播的总览数据和分钟数据合并存储，分钟数据格式为map，每次产生一条新的分钟数据时，会对总览数据的某个map中的数据进行累加，bin log为statement，所以占用空间非常大
				解决方案：分地区拆库，合并分钟数据，压缩格式 Kryo序列化（效率高，protobuf不支持原生的java对象序列化）
				数据结构是一个session跟多分钟的map数据吗？对的
			数据更新峰值高，实时和分钟级别数据高峰期qps 3w左右 ，持续时间8分钟，假设数据库upsert每秒5K，那么需要48分钟才能消费完，显然不可接受
				分流，主要指标数据和修正数据分流，主要峰值来源于同一同步的修正数据
				为什么现在qps只有上千？流量被切走了
			数据延迟在大促期间只能保证2min内，主播开播推流后，实时大屏入口迟迟看不见dashboard
				怎么解决？直接监听主播的开播信息
			session更新tps瓶颈，数据都放在一行，导致多线程更新表锁竞争压力大
				拆表0-100，拆分钟数据

		技术挑战
		高吞吐 每日6-8亿条宽表数据，直播大屏峰值5w wps
		低延迟 亚小时搬运亿级别数据，直播实时插入查询亚秒级别
		海量数据 千亿级别卖家数据，es集群商品数据700亿文档，6t存储

		原因：数据维度多 主播维度 主播*商品 主播*直播*商品 时间跨度大 数据要求实时性高，写入场景多，update较多，读取场景稍微少
				演化：
					cronjob 读hdfs 每日百万级数据
						异常数据不兼容
						吞吐量低
						不支持断点续传
					cronjob 调用PrestoSQL 每日亿级数据
						支持数据结构化
						大表支持差
					hive + spark + catalog + kafka + consumer
						支持dag形式的任务编排
						支持断点续传
						sql编写灵活，支持复杂查询
						吞吐量高，kafka容易横向拓展
							db负载高
								做不了读写分离，主读主写
								批插批写不太有效，因为主要是update
								限速削峰 不符合业务场景
								分库分表 已经分了
								tidb 需要付费
							bufferpool 脏页?
							mysql不适合大规模数据
							不适合写多读少场景
							表结构不易变更

			替代方案：kv为辅，hbase为主
			hbase lsm?
			hbase使用场景

		数据指标
			ID/VN 16台46G RDS， 总计10w OPS， 读写比2：1
			es 20亿直播间数据 700亿文档 6t存储 15k rps 12台
		方案选型
			mysql 因为更新特别多，会触发buffer pool频繁更新写脏页
			hbase 因为主要需要聚合 8t存储 峰值 2g磁盘io 1g网络io
				合理设计rowkey
				start stop减少扫描行数
				filter过滤数据

		解决方案
		线上问题

		后续优化

		问题：
			大屏为什么不做移动端？ 有的
			kv更新指标如何解决并发问题？	
			kol流量控制如何实现的自动增长？
			hbase存什么
			es存什么
			kv存什么 实时数据，榜单
			mysql存什么

			es/kv和db的数据一致性如何保证
				db作为兜底方案并没有强制要求一致性，只要主链路（写kv/es）没问题就行
			不同数据源对应的业务场景，为何这么选型
				直播大屏
					session item分钟级/总览数据 ES + db 需要商品名称模糊检索 需要按滑动窗口存储分钟级数据，而es能够支持筛选数组中的元素，类型为nested
					session 总览数据 kv + db 需要快速检索
					session 分钟级数据 kv + db kv的《hash》中的结构为views: {1111: xxx, 1112: xxx} key为时间戳，	kv设置为7天过期 不到10g
				内部看板
					session list 数据 ES  需要按照多个字段排序以及检索 和session总览数据一样，只是不一样的链路 sessionOverview到底存哪了？kv + es + db
					session rank kv zset 每个维度一个key，按照权值进行排序


			不同数据源的数据量
				Topic 1	ls_mart.ads_session_live_insight{region} 	3k （ID）	直播间总揽数据	DB +ToC Codis + ToB Codis  + HBASE
					RealTimeSessionOverviewConsumer 主链路写 KV + 旁路写DB热备   && LsSessionLeaderboardConsumer 更新某个地区某个维度的榜单，比如ID的GMV， order， item sold榜单，用zset  
				Topic 2	ls_mart.ads_session_live_insight_mi{region}	3k（ID）	直播间分钟数据	DB +ToC Codis + ToB Codis + HBASE
					RealTimeSessionMinuteConsumer 主链路写 KV 十分钟一条数据（和mysql类似对应字段hash） + 旁路写DB热备   && LsSessionLeaderboardConsumer 更新某个地区CCU（在线用户数）的榜单，因为要求实时数据，用zset  
					LsInternalSessionMinuteDataConsumer 写直播开播后总观看数的（为什么不用overview数据？）
				Topic 3	ls_mart.ads_session_item_live_insight{region} 	2w/s（ID）	商品总揽数据	ToC ES + DB(没看到写DB) + 	HBASE
					RealTimeItemOverviewConsumer 主链路写ES，用于直播间的商品列表  + LsItemLeaderboardConsumer 写kv 用于直播间的商品排行榜 + 写kv 用于记录某个地区某件商品的历史销量 + 直播下商品的overview kv LsItemAccumulateDataConsumer
				Topic 4	ls_mart.ads_session_item_live_insight_mi{region}	2w/s（ID）	商品分钟数据	ToC ES + DB + ? （应该没有写HBASE，因为这种实时数据没必要？）
					主链路写ES(RealTimeItemMinuteBatchConsumer) 用于更新ES中分钟级窗口（部分指标是5分钟窗口，部分指标是全部窗口） RealTimeItemMinuteConsumer写DB，十分钟一条数据，各维度指标用Kryo序列化（理由：只有java内部使用场景，和pb以及thrift序列化性能极高，反正都是不可读，选个效率高的）
					数据一致性（假设有校对job）

				Update link1	限速2w/s以内	DB热备 topic 1-4的数据，限速更新，KOL的数据单独消费队列不限速
				Update link2	>10w/s	直播间多指标展开存储更新，有写扩散，整体控制在 topic 1+2的 10倍
				Update link3	30w/s	topic 1 + 2 + 3的数据，做延迟打点监控，内部dashboard看板，写扩散多
				Update link4	4W/s	topic 3+4的数据update

			Hbase存什么
				离线数据
					streamer * item 10T 每月0.8T ? 为什么数据这么多
					streamer * session * munite  2T 每月0.16T 
					streamer * session * item < 1T

			kv
				直播商品分钟级数据（榜单+分钟级） 6g 2000w key qps 2w 跟kafka能对上
				直播分钟级数据 6g 800w key qps 3k

			

			线上问题
			你负责什么
				拆库拆表
				持久化逻辑
				后续持续优化
			遇到什么问题
				榜单消失 多线程对zset add del
				kafka poll time超时


			如何解决
			成效如何
			优化迭代

	线上问题
		https://confluence.shopee.io/pages/viewpage.action?pageId=1933551179

	jdk升级
		https://confluence.shopee.io/pages/viewpage.action?pageId=2332565944
		原有gc配置
		    -XX:+UseG1GC
		    -XX:+UnlockExperimentalVMOptions 解锁实验性 JVM 选项	允许使用实验性 GC 相关参数
		    -XX:MaxGCPauseMillis=70 目标最大 GC 暂停时间 70ms	G1 会调整回收策略，尽量满足这个暂停时间目标
		    -XX:G1NewSizePercent=35 新生代最小占比 35%	控制新生代初始大小，提高 YGC（年轻代 GC）吞吐量
		    -XX:G1MaxNewSizePercent=45 新生代最大占比 45%	限制新生代最大大小，防止老年代被挤占
		    -XX:G1MixedGCLiveThresholdPercent=85 老年代对象存活率高于 85% 时不参与 Mixed GC	避免回收高存活率的区域，提高 GC 效率
		    -XX:G1HeapWastePercent=5 允许 5% 的堆内存浪费	让 G1 保留一定的可用空间，提高回收灵活性
		    -XX:InitiatingHeapOccupancyPercent=35 堆占用率超过 35% 时启动并发 GC	让 GC 提前介入，减少 Full GC 可能性
		    -XX:+UseStringDeduplication 开启字符串去重	降低重复字符串的内存占用，减少堆压力
		    -XX:+ExplicitGCInvokesConcurrent System.gc() 触发并发 GC 而非 Full GC	避免 System.gc() 造成长时间 STW
		    -XX:+ParallelRefProcEnabled 并行处理引用	加快 GC 处理弱引用（如 SoftReference、WeakReference）
		    -XX:MaxMetaspaceSize=368m 设置 Metaspace 最大大小 368MB	限制类元数据占用空间，防止 Metaspace OOM
		    -XX:MetaspaceSize=368m 设置 Metaspace 初始大小 368MB	预留一定的类元数据空间，减少扩容时的开销
		痛点 服务偶发性STW时间达到秒级 ccweb 单次gc时长长达800ms，每分钟gc时长（疏散停顿：用于对象拷贝）长达近10s （实际4~5s）每分钟平均6~8次gc
		必要性 
			g1的缺点：
				老年代空间碎片严重，触发 Full GC
				预测失败，GC 来不及回收，YGC、Mixed GC 频繁退化为 Full GC
				回收效果差，老年代释放空间少，下一次 GC 更快触发 → GC 风暴
			面向C端服务不能容忍高延迟
		方案选型 jdk17 zgc jdk21 分代zgc，为了项目稳定性以及兼容现有中间件（spring kafka es等）版本选择了jdk17
		难点 升级需要适配各个中间件的版本，项目稳定性需要保证
		解决 从小型项目开始逐步升级，不同版本灰度发布
		    -XX:+UseZGC
		    -XX:ConcGCThreads="$con_gc_threads" 并发的线程数，越少越好，但是会影响并发阶段的耗时
		    -XX:ParallelGCThreads="$par_gc_threads" STW的线程数，越多越好，降低暂停时间
		    -XX:ZCollectionInterval="$zgc_interval"
		效果 
			偶发性的长时间STW消失，STW时间缩短到微妙级 ccweb 单次gc最大1ms，每分钟gc时长nms，每分钟30次gc左右
			每分钟gc次数 30次->个位数7,8次
			每分钟gc时长 4s-40s -> xxms-xxxms
			最大gc时长 40s -> 600ms



	hbase
		hbase版本 xx
		content_supply__ls_streamer_item_offline 每月增长 1.5t左右 现在19.6t 有time_dim字段，分1d 7d 30d
		content_supply__ls_session_minute_offline 每月增长 0.3t 现在3.8t
		除此以外还有mcn的item数据，分钟直播数据，短视频数据，短视频商品数据
		主要是一些持续增长的大批量的存量数据，存mysql不易拓展，业务场景也不需要复杂的查询条件
		row_key设计 基本都是根据业务场景的查询范围确定，比如按streamer维度就是streamer_id % 100 _ streamer_id _
		线上问题
			scan操作太多，cpu干限流了
			解决
				scan snapshot
				合并scan操作
				降频
			大量bulk load触发compaction 争抢正常IO
			解决
				调整bulk load策略，降频限制线程数


	mysql问题
		版本 5.7
		集群配置 一主两从 已经从phy迁移到rds
		监控团队集群维度+服务维度
		线上负载
			整体db综合读写比 2 ： 1 整体OPS在 70k ~ 160k左右 （都是主库）
			qps 主从比不同集群有的是 2： 1 有的是1 ： 1
			以下都是服务的容器配置
			cc web 1k qps 8c 16g 20i
			cc job 1w qps 16c 32g 22i
			ls job 6~7w qps 24c 32g 58i
			id supply_db_000000xx 80g * 16 qps 直播overview大概60mb * 100 * 16 minute数据 大概1.5g * 100 * 16（每月清理）
			id ls_db_000000xx 500g * 16 每个月25g增长 qps 主 2.8k 从 1.8k * 2 dr 2k buffer pool 50g左右
			vn ls_db_000000xx 400g * 8


		线上配置
		慢查询排查
			有一些慢查询超过1s，不影响正常业务，被认为是mysql的正常抖动

		线上问题
		可用性
			读写分离，不需要最新数据的读取都走从库
			索引优化、分库分表

	redis
		codis
		版本3.2
		主从配置 1从 未启用cluster 用sentinel进行故障转移
		allkeys lru策略
		主从lag 1s同步
		为什么不用集群？
		只使用 RDB（快照），AOF 没开启。
		但 RDB 似乎是手动触发，没有自动执行 save 规则。
		如果 Redis 崩溃，可能会丢失最近的变更，因为 RDB 只在上次 bgsave 之后保存了数据？

		数据
		一些实时数据包括

		codis vs cachecloud (封装了redis，提供各种管理监控功能的一款产品)
		Codis	3.2.6	基于redis原生，主从模式，用哨兵sentinel做故障转移	支持多db	社区已不活跃，redis版本多年无变化
		Cachecloud	6.2.6	基于redis-cluster原生实现	只支持db0	自研产品，有专门的研发团队，支持较好
		codis迁移到cachecloud
		迁移策略

		平台迁移：大部分集群可以通过平台自动迁移，减少业务侧的变更需求。
		业务改造迁移：特定情况（如多 DB 集群和无密码访问集群）需要业务侧介入和代码改动。
		迁移步骤

		创建 CacheCloud 集群：与原 Codis 集群配置相同，保证平滑切换。
		数据同步与一致性检查：确保数据在迁移过程中的完整性和一致性。
		域名与流量切换：更新 DNS 记录和 HAProxy 配置，切换流量到新集群。
		关闭旧集群与数据同步：完成迁移后关闭 Codis 集群和数据同步任务，确保新集群的独立运行。
		回滚计划

		准备充分的回滚策略，包括恢复旧的 DNS 设置和重启原 Codis 数据同步，确保在遇到不可预见问题时可以快速恢复服务。

		copi2 vs kv store
		自研的kv存储，兼容redis语法，基于raft进行节点同步

		线上问题
		大key问题？ 如果一个key的大小为1MB，每秒访问量为1000，那么每秒会产生1000MB的流量。这对于普通千兆网卡的服务器来说是灾难性的。
		规范数据类型，少用list只push的操作，设置过期时间，用del删除大key，但是4.0之前是O(n)复杂度会阻塞进程，需要写脚本分批删除
		大key监控+限流 缓存预热 本地缓存兜底
		缓存大促时有压力吗？如何应对
		数据量怎么样？未来如何拓展？ 
		通过Prometheus埋点监控热点Key，通过Prometheus查询接口精准过滤查询并定时更新进入本地内存，减轻redis负担，相比于全量缓存本地更加节省内存。


	es:
		版本
			7.8.1
		存储容量
			主+副本 7t左右 空间占用1年增长了1.5t左右 集群总共220T,显示磁盘占用了33T，可能是因为删除数据（docs.deleted 显示你的索引有 超过 7.9B（79 亿）条已删除的文档）以及其他占用
		参数调优
			主：副本=1:1
			异步落盘 5s异步刷translog
			禁止使用dynamic mapping，设置为strict，避免字段数量过多
			index_options没优化，因为好像所有字段都需要检索
		应用场景
			直播商品数据 直播下的商品列表排序 直播下的商品各个维度的分钟级窗口数据 （利用es能够对nested的某个子字段sum后直接排序的特性）
			直播数据 内部看板 直播列表需要各个维度筛选
			主播数据 内部看板 主播列表各个维度筛选
		性能水平
			到shard的请求分布均衡，没有热点shard
			数据分布均衡，33个分片，每个分片大小一致
			读写压力都不低，但以 Get Rate 为主（某个索引读取量特别高）。id地区商品分钟数大约为20k/s，其他地区像vn只有2k/s
			CPU 负载适中（最高 69%，平均 24%~48%）。
			Query 延迟（查询延迟）普遍较低，但偶尔有波动。一般是个位数ms级，偶尔波动到1s左右
			Refresh Rate（刷新频率） 也比较均匀，说明 ES 的 refresh_interval 设置合理，没有导致过多的索引开销。
		备选方案
			es
				索引极其占用空间
				写放大严重 需要merge segment
			ck 不适用于搜索场景
				不适用于高频更新 数据吞吐量大的时候，底层mergetree会很慢
			mongo
				搜索功能，聚合功能都不太行
		线上问题
			为了应对双12大流量（大约平时峰值的1.5倍）
		性能优化
			深度分页search after讲一讲
			异步落盘 5s异步刷translog
			批插批写 一次拉取1600条，24并发处理 get rate 380k/s index rate 261k/s
			模型优化
				只需要聚合不需要搜索，index设成false
				不需要算分，Norms设成false
				禁止使用dynamic mapping，避免字段数量过多
				优化index_options，控制在创建索引时哪些字段会添加到倒排索引
				根据业务需求，选择是否关闭_source 由于需要支持update, 我们这里不能关闭_source
			路由
				使用ES默认路由: 按文档id路由, 文档id : ${sessionId}-${itemId}
				确保ES分片均匀的分布在各个物理节点上
				设置合理的分片数
			cpu和io
				减少不必须的分词
				避免不需要的doc_values
				提高文档压缩率-文档写入时保证字段顺序
				提高文档压缩率-当字段值不变或字段为0时可以不写入 
				ES配置- refresh interval（牺牲搜索实时性） 5s刷新
				ES配置- translog async（牺牲可靠性）勾选为true
				根据业务需求, 选择是否使用ES自动生成的文档id 由于需要严格保证每个直播间-商品只能有1条数据，因此这里选择手动创建文档id: ${sessionId}-${itemId}
		流量压测
			上线前在liveish环境创建测试索引，引流live环境消息无积压消费12小时，对比es文档，观察消费情况
			压测时，选择业务低谷期，降低对业务的影响
			验证时，通过脚本及现成api，对正式索引和测试索引的数据进行批量验证，确保新consumer和旧consumer业务逻辑一致
			上线时，由于商品消息消费是幂等的，因此通过先双写(新旧consumer同时写正式索引)->验证->关闭旧consumer方式来实现平滑切换。在业务低谷期按地区挨个切换，优先切换流量小的地区
		拓展计划
			已删除文档过多，准备定期force merge segment
			进一步优化index_options 控制不必要的倒排索


	kafka
		3/3为例
		session流量 3k - 1.5w
		商品 1w - 7w
		订单 100 - 3k
		总流量 4w - 14w
		latency de生成/发送延迟都是十几秒
		be消费也是十几秒（除no kol mysql有限流以外）

		线上问题：
		一次性拉取过多，导致消费速度过慢超时，超过max.poll.interval.ms发生rebalance，重新跟broker建立链接之后，又从原先的offset开始消费
		相同broker，不同topic，group id设置相同,但是topic鉴权不相同，导致rebalance之后无法消费，消息堆积


	数据指标
		db占用
		es占用
		kafka

	性能优化
		https://confluence.shopee.io/pages/viewpage.action?pageId=2284141477
		直播大屏es：https://confluence.shopee.io/pages/viewpage.action?pageId=2153729220



特征平台

	数据源 -> 数据 -> 整合处理 -> kv(用作特征匹配判定根据userId查相应特征 userId - 多特征key -> value)    通过特征圈选人群包、传入userId判定特征
					-> es（多特征到userId的映射） 



	用户特征  来源 应用场景



	资源申请
		copi2：	key个数 7000w
				key类型 hash 
				key长度 20byte
				字段数 200
				字段长度 10byte
				value长度 10byte
				总计：500G

		运行模式：高性能/大存储
		预估总容量（key+value+buffer）1000G
		预估QPS：20000 = 1000（下游服务qps） * 8（下游服务个数） read 120（特征接入任务数） * 100（单个任务qps） write
		使用命令：hget hgetall hset
		读/写命令分布占比：40% 60%
		最大value长度 200 * 3byte
		平均value长度 10byte
		key总数量 7000w
		平均key长度 10byte
		平均TTL -1
		客户端并发连接数 100
		预期读命令P99 10ms
		预期写命令P99 10ms
		是否有plan B 有
		能否容忍机房故障 不能


		es: 单文档大小 =  200 * 4byte(普通数字) + 20 * 50字符 * 3byte（utf8字符串） = 3800byte / 3.8kB
			索引数 = 2
			文档总数 = 2000w(用户特征) + 5000w(视频特征)
			源数据大小 = 3.8kB * 7000w = 266GB 
			总容量大小 = 源数据大小 * (1 + 副本数量) * (1 + 索引开销) / （1 - 操作系统预留）/ (1 - 内部任务开销) * (1 + 预留空间) 
					 = 266GB * (1 + 1) * (1 + 0.1) / (1 - 0.05) / (1 - 0.2) * (1 + 0.5)
					 = 266GB * 4.35
					 = 1.2TB


		版本号：7.8.1
		业务场景描述：存用户/视频特征数据
		预估容量（总容量，包含副本）： 1.2TB （600G + 副本）
		副本数：1
		索引数量：2（用户表 + 视频表）
		文档数量：7000W = 2000w(用户总数) + 5000w(视频总数)
		index rate（写入速率，docs/s，index rate=写入文档数*副本数）：100（单个任务docs/s） * 100(任务数) * 1（副本数） = 10000
		单文档平均大小（kb）：3.8kb
		search rate（查询速率，search rate=查询qps*单次qps所查询所有索引的主分片之和，自定义路由除外）：100（qps） * 200（分片数） = 20000
		查询类型（term match，agg....等等）：term match
		整个集群总的分片数：2索引*100分片每个索引=200
		PS：1.es索引设计注意点：1.单分片容量尽量不要超过20G，2.单分片doc条数硬上限20亿条
		        2.各种复杂的分词逻辑尽量不要用es去完成，比较损耗es性能，尤其是ngram分词器或者一些第三方不可控的分词器。

		ES和CK都可以做人群圈选，但各自优势不同：

		CK更适合多维条件组合、高并发、海量数据实时聚合场景。典型如实时用户画像、精准营销、实时推荐、复杂圈选条件。
		ES更适合文本搜索、模糊匹配、检索类需求场景，如日志全文搜索、商品检索、关键词模糊匹配等场景。
		如果纯粹做人群圈选（尤其复杂圈选），CK优势明显，实际生产环境下企业更普遍采用ClickHouse来实现。

		除了人群圈选还有模糊搜索的功能，其次较多为离线数据，性能要求不高

		Redis：

		改进 key 结构：减少 hgetall 依赖，优化 hash 结构，提高 pipeline 传输效率。
		优化存储：调整 hash-max-ziplist 配置，避免 hash 过大影响查询效率。
		高可用方案：双机房 Cluster 或 Proxy 方案，避免单点故障。
		ES：

		调整分片策略：如果 term match 查询居多，routing 到 userId 可能减少分片查询数。
		优化索引：doc_values 关闭不常用字段，refresh_interval 增加至 30s 降低 merge 负担。
		冷热数据策略：减少 hot 索引查询压力，提高 search 响应时间。
			Elasticsearch 自动根据时间动态迁移数据的核心是 Index Lifecycle Management (ILM)，它可以按时间自动将数据从 Hot 节点迁移到 Warm/Cold 节点，并最终删除老数据（也可以不删除）
		ClickHouse 作为 Plan B

		如果后续圈选需求变复杂（如 实时计算 场景），ClickHouse 可作为优化方案。



有道少儿英语订单系统拆分


项目介绍

这个订单异步化改造主要是对我们订单原有基于线程池的方案就行了一些优化，因为之前我们订单下单后的一些逻辑，包括开课、短信、活动、物流等，都是在用户支付完成后，在线程池中进行的。

这样就可能会导致一些问题：

首先是当订单量如果突增的话，大量订单会导致线程池拒绝的风险，
二是之前的过程中，对于业务执行失败的订单，缺少了异常记录的操作，
三是所有不同类型的业务耦合性太强，希望去对他进行一个拆分，比如说一笔订单支付成功后，活动的执行失败不应该影响后续开课的业务执行。

因此对他进行了优化，当时采用了canal+kafka的方式，我们这边订单支付后的回调接口，是将订单状态由0改为1，所以canal只需要监听订单表的变化订单；发送到kafka中，对于这个topic我创建了一个负责清洗的消费者，会解析canal发送的消息，只过滤状态值从0变为1的订单数据；

之后将订单id再次进行转发；

另外我是将订单后置的履约业务拆分为独立的互不影响的几个子业务，将一些共有的方法抽象为一个抽象类，其中提供了模板方法，每个子业务只需要继承这个抽象类，在其中定义自己的业务码，编写各自的业务逻辑就可以很好的扩展，也方便了以后其他同事的开发。

每个子业务都作为一个消费者组来对支付成功的订单topic进行监听，接受到以后通过orderId就可以还原订单、商品、用户的一些数据信息，通过在redis分布式锁+mongo中记录执行结果，来确保原子性避免重复消费（因为对每个子业务定义了业务码，可以区分）。其次对于执行异常的订单，会根据订单id、业务码记录下来，通过定时任务补偿的方式确保最终一致性。


可能后续会问道或者可以在介绍中补充的问题：

线程池相关，各个参数和提交流程
canal的机制，mysql数据同步
一致性方案
分布式锁
订单量特别大怎么办：读写分离、缓存、分库分表（https://blog.csdn.net/weixin_48182198/article/details/108475822、）


订单系统
	订单流程 下单：不同入口（h5，公众号，app，小程序）->创建订单->支付：不同渠道（支付宝、微信、苹果）->支付回调->履约（开课、发短信、发消息、优惠劵、物流）
	订单信息 商品信息 + 用户信息 + 支付信息 + 履约信息 + 物流信息
	订单状态 初始状态->支付->（支付成功、失败、超时）->（开课中、成功、失败）->（退款中、成功、失败）
	订单实际方案 基于base法则 尽量确保最终一致性 线程池+定时任务xxljob
	订单存在问题 线程池问题：订单量激增导致线程池拒绝、业务没有隔离开、大量线程池容易导致fullgc、没有补偿机制和监控 定时任务问题：@schedule单线程导致任务饿死、任务没有设置优先级、没有监控和报警机制 其他问题：统计类需求和用户 
	重构方案 线程池->kafka消息队列 重试补偿机制 定时任务->xxljob 任务增加逆向操作流程
	遇到问题
	分布式解决方案
	canal
	kafka
	订单数据怎么过滤的？kafka的生产者从哪来？ canal server配置来的
	实际kafka的配置？ 6 partition 有没主从 不知道
	canal的角色 监听的是库还是表 mysql的整个bin log 但是可以过 滤发出的消息 表级别过滤 如何保证有序 消息没有强关联不需要有序，而且每个partition中的消息也是顺序的 消息格式是怎样的 一条消息多大
	kafka 配置如何 多少个topic 一个topic，后续履约流程每个流程一个消费者组，消息订阅发布模式 多少个partition 反正>=3 如何保证接受的消息有序 消费者concurrency和partition保持一致
	消费出现问题如何重试，应该怎么办 首先kafka不支持重试 其次应该保证消费的幂等性，接口层面记录执行日志，以及加上分布式锁
	消息消费出现了并发问题如何解决
			redis分布式锁
	订单系统如何设计 
	表设计问题 订单id 商品快照 其他信息
	状态流转问题 支付 开课 退款
	事务一致性问题 spring事务 单库事务 后续服务拆分就需要分布式事务 seata/ JTA
	数量大了怎么办 分库分表 
	

如何保证代码质量
	统一规范 阿里java规范
	覆盖单元测试 重要接口跟工具类有对应的单测用例
	接口/集成测试（自动化流程） qa在test环境有每日的自动化测试用例
	静态代码扫描 sonarlint扫描代码，未满足条件不允许提交
	code review代码评审机制 merge到release前需要核心成员review代码
	分支管理清晰 不同环境的分支命名规范，git使用规范

大促前的准备
	核心链路梳理 核心链路为直播数据展示链路
	容量评估/压测 评估大促期间直播数据的峰值以及写入量，用上个月的峰值 * 1.5/2 压测都是手动
	缓存预热、热点数据隔离 因为db跟kv不是cache aside的关系，而是异构存储，所以不需要缓存预热，db倒是可以把kol的数据预加载到buffer pool中，es的数据可以通过提前索引，force merge降低存储，提升性能
	限流降级策略 多数据源主链路为es/kv 备用链路为mysql kol数据链路可以降级，确保kol的直播看板服务可用
	灰度发布机制 大促期间封板，如果非有新功能在大促期间发布，可以在某些容器上进行灰度发布，怎么灰度？前端跟后端如何灰度同步？通过ab团队配置，前端不同用户拉取不同版本显示不同样式，后端可以走金丝雀发布（如果需要观察新服务是否稳定，比如写链路优化改批插批写）
	告警 监控升级 建立包括某些业务指标（接口）跟技术中间件指标（es，kafka等）的告警看板，admin平台上可以查看某一天产生的直播数据以及转化率的业务指标
	应急预案准备 大促前根据压测结果，对某些关键功能进行升级（代码升级：优化逻辑，配置升级：加机器）制定应急预案，确定线上oncall的责任人
	异地双活 没做，因为我们是根据区域定制化配置的，新加坡机房跟美东机房的数据做了隔离


跨团队沟通
	换位思考 了解对方的难点，陈述项目的重要性
	对齐目标 站在整体业务的角度去推进项目
	借助机制 通过项目会推进，拉齐leader推动


待补充的部分
	
	服务实例动态扩缩容 
		pod层面根据cpu io动态拓展
		node层面当pod资源不足时动态拓展
