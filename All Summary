
自我介绍：
	面试官你好，我叫吕超毅，17年毕业于华中科技大学软件工程学院，现在在shopee video supply团队负责Java开发

	我们的团队职责主要有两块：
	业务层面
		video的创作者相关，包括：创作工具、内容变现、流量激励、任务相关、主播助手
		live相关，面相卖家的内容，包括：直播大屏、直播诊断、AIGC
		内容供给相关，包括：短视频图片的爬取
	技术层面
		数据中台的职责，提供数据服务，包括主播端的实时看板、以及面向运营的一些离线数据的统计和可视化平台

	我在团队中的职责也分两块(核心开发 + 技术方案设计 + 线上保障)：
	业务层面
		日常业务owner项目以及代码开发
			技术选型
			输出技术文档
			资源申请
			风险评估
			技术分享
	技术层面
		项目的维护以及架构优化
			日常指标监控
			大促期间的问题定位
			系统的稳定性优化
			告警监控问题处理

	业务团队介绍
		团队职责
			创作者内容供给 主播流量激励、任务体系、创作周报
			卖家内容供给 直播实时大屏 
			视频图片内容供给 商品图爬取，aigc图片产出
			数据中台 实时数据、离线数据 聚合看板
		数据范围
			直播数据
			视频数据
			商品数据
			创作者数据
		


	之前主要参与过的项目有：直播大屏系统（基本的业务代码， 日常的报警监控，es的优化，分库分表）、特征平台（负责架构设计，项目搭建，风险评估），有道少儿英语的订单系统（主要负责订单系统的重构和优化）

	主要的技术栈有：Java全家桶（包括Spring、SpringBoot之类的），数据库包括：MySQL、Redis、MongoDB等，其他中间件的话：kafka、ElasticSearch用过一些


	业务系统
		creator-centor 创作者入口
			创作者数据处理 core data bin log
			作者推荐服务：给用户推荐作者，帮助作者涨粉
			榜单服务：给用户展示视频排名、作者排名榜单
			任务系统：让创作者完成一系列任务，提高创作者积极性
			课程服务：给创作者提供课程服务，增加专业知识
			带货数据分析：给作者提供视频带货订单分析
			权益流量包：给作者发放流量包，失活作者召回
		creator-market 撮合平台，创作者洽谈合作的平台
			邀约系统：同谷歌日历
			创作者信息维护
			通知提醒服务
		admin-platform
			流量扶持：发布活动，给某些大v的视频增加流量
			保量数据分析：数仓数据聚合筛选同步

	基础服务
		spex4j rpc服务
		config-server 配置中心
		data-park 数据特征标签服务
		creator-payment 支付服务


重点项目介绍：
	
	架构图/项目结构图 todo

	数据流转

	组件相关问题



	kol激励

-----------------------------------------------------

好的，下面详细梳理“任务激励”在你项目中的**完整流程、核心逻辑、各模块分工**，并结合代码结构说明每一步的作用和实现方式。

---

# 1. 总体流程图

```
[任务配置] → [任务领取/进度] → [任务完成检测] → [奖励配置查找] → [奖励计算] → [奖励发放] → [奖励状态更新/通知]
```

---

# 2. 主要模块划分

1. **任务配置与管理**
   - 任务基础配置（如类型、周期、奖励规则等）
   - 任务实例（用户领取/进度/完成状态）

2. **奖励配置与管理**
   - 奖励类型（现金/流量/实物等）
   - 奖励触发规则（完成/事件/进度）

3. **任务进度与完成检测**
   - 任务进度实时更新
   - 完成状态检测与触发

4. **奖励计算与分层处理**
   - 奖励金额/内容计算
   - 多层级奖励处理（如达标多档奖励）

5. **奖励发放与异步处理**
   - 发放接口调用（如支付系统、流量系统）
   - 发放结果回写与重试

6. **奖励状态管理与通知**
   - 奖励状态流转
   - 用户通知（如PN、AR）

---

# 3. 详细流程与代码实现

## 3.1 任务配置与管理

- **配置表/实体**：`TaskBaseConfigEntity`
- **管理类**：`ITaskBaseConfigRepository`
- **作用**：定义任务类型、周期、奖励规则、可领取人群等。

## 3.2 用户任务实例

- **实体**：`CreatorTaskEntity`
- **管理类**：`ICreatorTaskRepository`
- **作用**：记录每个用户的任务领取、进度、完成状态等。

## 3.3 奖励配置与管理

- **配置表/实体**：`TaskRewardConfigEntity`
- **管理类**：`ITaskRewardConfigRepository`
- **作用**：定义每个任务的奖励类型、金额、触发方式、分层规则等。

## 3.4 任务进度与完成检测

- **进度更新**：用户行为（如发帖、点赞）触发进度变更，更新 `CreatorTaskEntity` 的进度字段。
- **完成检测**：当进度达到目标，任务状态变为 `DONE`，触发奖励流程。

## 3.5 奖励计算与分层处理

- **奖励计算主入口**：`CreatorTaskService` 相关方法
  - `calcBonusInfo4CreatorTask`、`calcBonusInfo4Task`、`calcBonusInfo4ViralTask`
- **分层奖励处理**：`BonusRewardProcessor.getRewardInfo4Layered`
  - 支持多档奖励，自动扣除已发放的低档奖励，避免重复发放。

## 3.6 奖励发放与异步处理

- **奖励发放主入口**：`BonusRewardGrant.grant`
  - 检查奖励类型和状态，构造发放请求，调用支付系统接口。
- **异步/延迟发放**：`RewardProcessor`（抽象类，含线程池和延迟队列）
  - 支持奖励的异步发放、失败重试。

## 3.7 奖励状态管理与通知

- **奖励记录表/实体**：`CreatorTaskRewardEntity`
- **管理类**：`ICreatorTaskRewardRepository`
- **状态流转**：`RECEIVING` → `DONE`（发放成功）或 `FAILED`（失败）
- **用户通知**：`CreatorTaskNoticeService` 负责PN/AR推送

---

# 4. 代码结构与关键类

| 模块 | 主要类/接口 | 说明 |
|------|------------|------|
| 任务配置 | `TaskBaseConfigEntity`, `ITaskBaseConfigRepository` | 任务基础信息 |
| 用户任务 | `CreatorTaskEntity`, `ICreatorTaskRepository` | 用户任务实例 |
| 奖励配置 | `TaskRewardConfigEntity`, `ITaskRewardConfigRepository` | 奖励规则 |
| 奖励记录 | `CreatorTaskRewardEntity`, `ICreatorTaskRewardRepository` | 奖励发放记录 |
| 奖励计算 | `CreatorTaskService`, `BonusRewardProcessor` | 奖励金额/内容计算 |
| 奖励发放 | `BonusRewardGrant`, `RewardProcessor` | 奖励发放、异步处理 |
| 状态管理 | `CreatorTaskRewardEntity`, `ICreatorTaskRewardRepository` | 奖励状态流转 |
| 通知 | `CreatorTaskNoticeService` | 用户通知 |

---

# 5. 详细时序举例

1. **任务配置**：运营在后台配置任务和奖励规则。
2. **用户领取/参与任务**：用户行为（如发帖）自动领取任务，生成 `CreatorTaskEntity`。
3. **进度更新**：每次用户行为，更新任务进度。
4. **完成检测**：进度达标，任务状态变为 `DONE`。
5. **奖励配置查找**：查找该任务的奖励配置（如现金奖励、流量奖励）。
6. **奖励计算**：根据配置、用户信息、分层规则，计算应发放奖励金额。
7. **奖励发放**：
   - 构造发放请求（如 ShopeePay）
   - 调用支付系统接口
   - 发放成功后，更新奖励状态为 `DONE`
   - 发放失败，进入重试队列
8. **奖励状态管理**：奖励状态流转，记录发放时间、金额等。
9. **用户通知**：奖励发放后，推送PN/AR通知用户。

---

# 6. 典型代码片段

**奖励计算：**
```java
private Map<Long, BonusInfo> calcBonusInfo4CreatorTask(List<CreatorTaskRewardEntity> creatorTaskRewardEntityList) {
    // 遍历奖励记录，累加每个任务的奖励金额
}
```

**奖励发放：**
```java
public int grant(CreatorTaskRewardEntity rewardEntity, CreatorTaskEntity creatorTaskEntity) {
    // 检查奖励类型和状态
    // 构造发放请求
    // 调用支付系统接口
    // 更新奖励状态
}
```

**异步发放：**
```java
@PostConstruct
public void init() {
    new Thread(() -> {
        for (; ; ) {
            DelayGrantTask task = delayGrantTasks.take();
            THREAD_POOL_EXECUTOR.execute(task);
        }
    }).start();
}
```

---

# 7. 典型激励类型

- **现金奖励（BONUS）**：发放到 ShopeePay 等钱包
- **流量奖励（TRAFFIC_SUPPORT）**：发放流量包/曝光资源
- **多层级奖励**：如达标多档奖励，自动扣除已发放的低档奖励

---

# 8. 配置与扩展

- 奖励金额、发放规则、发放方式都可通过 spexconfig 和奖励配置表灵活调整。
- 支持多种奖励类型和多层级奖励，避免重复发放。

---

# 9. 典型异常与处理

- 发放失败自动重试
- 状态异常报警
- 奖励金额超限报警（如 `BonusRewardAlarmAmountConfig` spexconfig）

---

如需某一环节的详细代码、表结构、配置样例或具体业务场景举例，请随时补充！



好的，下面详细梳理**完整的任务激励（奖励）发放流程**，结合你项目的实际代码结构和业务逻辑，分为**前置准备、触发、计算、发放、回写、通知**等环节。

---

## 1. 前置准备

### 1.1 任务与奖励配置
- 运营在后台配置任务（如发帖、视频、带货等）和奖励（如现金、流量、实物）。
- 配置内容包括：任务类型、周期、目标、奖励类型、奖励金额、触发方式（完成/事件/进度）、分层规则等。
- 配置表：`TaskBaseConfigEntity`、`TaskRewardConfigEntity`

### 1.2 用户任务实例
- 用户领取任务或自动分配任务，生成 `CreatorTaskEntity`（记录用户任务进度、状态等）。

---

## 2. 任务进度与完成

### 2.1 进度更新
- 用户行为（如发帖、点赞、带货等）触发任务进度更新，写入 `CreatorTaskEntity`。

### 2.2 完成检测
- 系统检测任务进度是否达标，若达标则将任务状态置为 `DONE`。

---

## 3. 奖励触发与计算

### 3.1 奖励触发
- 检查该任务是否有奖励配置（如现金奖励、流量奖励）。
- 检查奖励的触发类型（如 `TASK_COMPLETED`、`EVENT`、`PROGRESS`）。
- 只有满足触发条件（如任务状态为 `DONE`），才会进入奖励发放流程。

### 3.2 奖励计算
- 读取奖励配置，结合用户信息、任务完成情况，计算应发放的奖励金额/内容。
- 支持多层级奖励（如达标多档奖励），自动扣除已发放的低档奖励，避免重复发放。
- 主要代码：`BonusRewardProcessor.getRewardInfo4Layered`、`calcBonusInfo4Task` 等。

---

## 4. 奖励发放

### 4.1 奖励发放入口
- 由 `RewardProcessor` 或 `BonusRewardGrant` 负责实际发放。
- 检查奖励类型（如 `BONUS` 现金奖励）和奖励状态（如 `RECEIVING`）。

### 4.2 发放流程
1. 构造发放请求（如 ShopeePay 钱包发放请求）。
2. 调用支付系统/流量系统等外部接口，发放奖励。
3. 发放成功后，更新奖励记录状态为 `DONE`，记录发放时间、金额等。
4. 发放失败则进入重试队列，异步重试。

- 主要代码：`BonusRewardGrant.grant`、`PaymentProxy.disburseBonus`

---

## 5. 奖励状态回写与通知

### 5.1 状态回写
- 奖励发放成功后，更新 `CreatorTaskRewardEntity` 的状态为 `DONE`，并记录发放时间、金额等。
- 发放失败则记录为 `FAILED`，并可自动重试。

### 5.2 用户通知
- 奖励发放后，调用 `CreatorTaskNoticeService` 推送 PN/AR 通知用户，告知奖励到账。

---

## 6. 典型时序图

```
[用户行为] → [任务进度更新] → [任务完成检测] → [奖励触发] → [奖励计算] → [奖励发放] → [奖励状态回写] → [用户通知]
```

---

## 7. 关键代码链路

- 任务进度/完成检测：`CreatorTaskService`、`CreatorTaskEntity`
- 奖励配置查找：`TaskRewardConfigEntity`、`ITaskRewardConfigRepository`
- 奖励计算：`BonusRewardProcessor`、`calcBonusInfo4Task`
- 奖励发放：`BonusRewardGrant`、`RewardProcessor`、`PaymentProxy`
- 状态回写：`ICreatorTaskRewardRepository`
- 用户通知：`CreatorTaskNoticeService`

---

## 8. 典型异常处理

- 发放失败自动重试
- 状态异常报警
- 奖励金额超限报警（如 spexconfig 配置）

---

## 9. 总结

**任务激励发放流程**就是：  
任务配置 → 用户进度 → 完成检测 → 奖励配置查找 → 奖励计算 → 发放奖励（外部接口）→ 状态回写 → 用户通知。

每一步都有专门的表、实体、服务类负责，支持多种奖励类型、分层奖励、异步发放和重试，保证奖励准确、及时、安全地发放到用户手中。

如需某一环节的详细代码、表结构、配置样例或具体业务场景举例，请随时补充！






-----------------------------------------------------
	


	直播大屏
		背景目标
			卖家？需要在dashboard上查看直播的相关数据指标，包括GMV、like、item sold之类，类似与抖音直播看板 // todo 业务场景不同，偏向于社交类型的直播，需要计算礼物打赏，社交属性直播（如抖音）看重互动性、实时性与个性化推荐，强调用户参与；电商直播则更关注转化率、商品流量与交易数据，强调稳定性、营销与运营支撑。这决定了二者在大屏系统上的核心指标侧重、技术架构、数据采集与计算方式**存在明显差异
		特点
			能够承载高吞吐量5w wps的直播数据，拥有kv、mysql、es、hbase多数据源存储方案，系统高可用，能够进行亚秒级的近实时直播商品数据的查询，为创作者提供了丰富的多口径多维度的直播以及商品数据展示
		难点
			高并发&低延时
				限流 限流配置项保证核心链路可用
				Kafka	增加分区、压缩、批量发送	扛写入
				消费端	多线程 + 批写 + 状态合并	降 DB 压力
				Redis	Pipeline + Lua + 分片	高并发写入
				DB es 写入	批查批写 分库分表	减缓 IO 压力
			数据稳定性
				 1. 定时对账（异构校验）
				每小时/每天有校验任务对 Redis vs MySQL vs ES 的数据：
				比如：主播 room_id=123，分钟级的 viewer_count 在三个系统里是否一致；
				差异数据打日志或进入补偿队列重新修复；

				典型技术：Flink Batch / Spark / 自研校验脚本；

				2. 数据完整性监控
				每分钟监控 Kafka 消费 TPS 与 Redis 写入成功数、ES 索引数是否匹配；
				超出阈值（比如掉 0 或偏差 >5%）就告警或触发补偿任务；

				3. 关键链路数据持久化 + 回溯补偿
				Kafka topic 数据保留 3-7 天；
				万一 Redis 数据掉了，可以用 Kafka replay 或数据库反推补全；

			冷热数据分层
				热数据 榜单 窗口数据在kv es
				冷数据 直播
			服务可靠性保障
				熔断 下游服务挂了之后，逻辑走旁路，减轻对下游服务的影响
				降级 某些中间件挂掉，优先保证kol主要链路通畅，比如kv挂了，可以从本地缓存取数作为兜底（一致性？）
				限流 上游数据流量大了，可以通过限流器限制消费速度，主要针对非实时的db查询
				全链路压测 流量隔离（打标） 数据隔离（专门的表） 依赖隔离 限流熔断保护机制 // todo

		问题
			直播数据按地区拆库之后，id地区仍有大量bin log,2天时间1.2t 原因：单场直播的总览数据和分钟数据合并存储，分钟数据格式为map，每次产生一条新的分钟数据时，会对总览数据的某个map中的数据进行累加，bin log为statement，所以占用空间非常大
				解决方案：分地区拆库，合并分钟数据，压缩格式 Kryo序列化（效率高，protobuf不支持原生的java对象序列化） // todo
				数据结构是一个session跟多分钟的map数据吗？对的
			数据更新峰值高，实时和分钟级别数据高峰期qps 3w左右 ，持续时间8分钟，假设数据库upsert每秒5K，那么需要48分钟才能消费完，显然不可接受
				分流，主要指标数据和修正数据分流，主要峰值来源于同一同步的修正数据
				为什么现在qps只有上千？流量被切走了
			数据延迟在大促期间只能保证2min内，主播开播推流后，实时大屏入口迟迟看不见dashboard
				怎么解决？直接监听主播的开播信息
			session更新tps瓶颈，数据都放在一行，导致多线程更新表锁竞争压力大
				拆表0-100，拆分钟数据

		技术挑战
		高吞吐 每日6-8亿条宽表数据，直播大屏峰值5w wps
		低延迟 亚小时搬运亿级别数据，直播实时插入查询亚秒级别
		海量数据 千亿级别卖家数据（doc数量+nested数量，因为nested在lucene里也是一个单独文档存储），es集群商品数据700亿文档，6t存储

		原因：数据维度多 主播维度 主播*商品 主播*直播*商品 时间跨度大 数据要求实时性高，写入场景多，update较多，读取场景稍微少
				演化：
					cronjob 读hdfs 每日百万级数据
						异常数据不兼容
						吞吐量低
						不支持断点续传
					cronjob 调用PrestoSQL 每日亿级数据
						支持数据结构化
						大表支持差
					hive + spark + catalog + kafka + consumer
						支持dag形式的任务编排
						支持断点续传
						sql编写灵活，支持复杂查询
						吞吐量高，kafka容易横向拓展
							db负载高
								做不了读写分离，主读主写
								批插批写不太有效，因为主要是update
								限速削峰 不符合业务场景
								分库分表 已经分了
								tidb 需要付费
							bufferpool 脏页?
							mysql不适合大规模数据 hbase // todo
							不适合写多读少场景
							表结构不易变更

			替代方案：kv为辅，hbase为主
			hbase lsm? 顺序刷盘代替随机读写
			hbase使用场景

		数据指标
			ID/VN 16台46G RDS， 总计10w OPS， 读写比2：1
			es 20亿直播间数据 700亿文档 6t存储 15k rps 12台
		方案选型
			mysql 因为更新特别多，会触发buffer pool频繁更新写脏页
			hbase 因为主要需要聚合 8t存储 峰值 2g磁盘io 1g网络io
				合理设计rowkey
				start stop减少扫描行数
				filter过滤数据

		解决方案
		线上问题

		后续优化
			性能优化（缓存、批处理、限流）es性能优化，存储空间优化
			架构升级（服务解耦、平台化、可配置化）
			可观测性（监控、告警、链路追踪）全链路压测
			稳定性（幂等、重试、降级）
			功能拓展（更多数据源、更多场景）更多数据源，更详尽的内容呈现
			工程效率（任务编排、低代码平台）


		问题：
			大屏为什么不做移动端？ 有的
			kv更新指标如何解决并发问题？因为是先读后写，所以并发场景都是对hash里的不同字段进行更新
			kol流量控制如何实现的自动增长？
			hbase存什么
			es存什么
			kv存什么 实时数据，榜单
			mysql存什么

			es/kv和db的数据一致性如何保证
				db作为兜底方案并没有强制要求一致性，只要主链路（写kv/es）没问题就行
			不同数据源对应的业务场景，为何这么选型
				直播大屏
					session item分钟级/总览数据 ES + db 需要商品名称模糊检索 需要按滑动窗口存储分钟级数据，而es能够支持筛选数组中的元素，类型为nested
					session 总览数据 kv + db 需要快速检索
					session 分钟级数据 kv + db kv的《hash》中的结构为views: {1111: xxx, 1112: xxx} key为时间戳，	kv设置为7天过期 不到10g
				内部看板
					session list 数据 ES  需要按照多个字段排序以及检索 和session总览数据一样，只是不一样的链路 sessionOverview到底存哪了？kv + es + db
					session rank kv zset 每个维度一个key，按照权值进行排序


			不同数据源的数据量
				Topic 1	ls_mart.ads_session_live_insight{region} 	3k （ID）	直播间总揽数据	DB +ToC Codis + ToB Codis  + HBASE
					RealTimeSessionOverviewConsumer 主链路写 KV + 旁路写DB热备   && LsSessionLeaderboardConsumer 更新某个地区某个维度的榜单，比如ID的GMV， order， item sold榜单，用zset  
				Topic 2	ls_mart.ads_session_live_insight_mi{region}	3k（ID）	直播间分钟数据	DB +ToC Codis + ToB Codis + HBASE
					RealTimeSessionMinuteConsumer 主链路写 KV 十分钟一条数据（和mysql类似对应字段hash） + 旁路写DB热备   && LsSessionLeaderboardConsumer 更新某个地区CCU（在线用户数）的榜单，因为要求实时数据，用zset  
					LsInternalSessionMinuteDataConsumer 写直播开播后总观看数的（为什么不用overview数据？）
				Topic 3	ls_mart.ads_session_item_live_insight{region} 	2w/s（ID）	商品总揽数据	ToC ES + DB(没看到写DB) + 	HBASE
					RealTimeItemOverviewConsumer 主链路写ES，用于直播间的商品列表  + LsItemLeaderboardConsumer 写kv 用于直播间的商品排行榜 + 写kv 用于记录某个地区某件商品的历史销量 + 直播下商品的overview kv LsItemAccumulateDataConsumer
				Topic 4	ls_mart.ads_session_item_live_insight_mi{region}	2w/s（ID）	商品分钟数据	ToC ES + DB + ? （应该没有写HBASE，因为这种实时数据没必要？）
					主链路写ES(RealTimeItemMinuteBatchConsumer) 用于更新ES中分钟级窗口（部分指标是5分钟窗口，部分指标是全部窗口） RealTimeItemMinuteConsumer写DB，十分钟一条数据，各维度指标用Kryo序列化（理由：只有java内部使用场景，和pb以及thrift序列化性能极高，反正都是不可读，选个效率高的）
					数据一致性（假设有校对job）

				Update link1	限速2w/s以内	DB热备 topic 1-4的数据，限速更新，KOL的数据单独消费队列不限速
				Update link2	>10w/s	直播间多指标展开存储更新，有写扩散，整体控制在 topic 1+2的 10倍
				Update link3	30w/s	topic 1 + 2 + 3的数据，做延迟打点监控，内部dashboard看板，写扩散多
				Update link4	4W/s	topic 3+4的数据update

			Hbase存什么
				离线数据
					streamer * item 10T 每月0.8T ? 为什么数据这么多
					streamer * session * munite  2T 每月0.16T 
					streamer * session * item < 1T

			kv
				直播商品分钟级数据（榜单+分钟级） 6g 2000w key qps 2w 跟kafka能对上
				直播分钟级数据 6g 800w key qps 3k

			

			线上问题
			你负责什么
				拆库拆表
				持久化逻辑
				后续持续优化
			遇到什么问题
				榜单消失 多线程对zset add del
				kafka poll time超时


			如何解决
			成效如何
			优化迭代


	大促期间高可用
		前端降级&缓存兜底，数据延迟时显示之前的数据，非核心指标进行延迟更新
		系统高可用：多机房支持流量切换，读数据主要来自kv跟es，防止请求流量打挂db
		压测&容量规划：大促前压测探底
		报警&限流机制：业务指标+技术指标，超过阈值自动报警+降级保证kol可用


	线上问题
		https://confluence.shopee.io/pages/viewpage.action?pageId=1933551179

	jdk升级
		https://confluence.shopee.io/pages/viewpage.action?pageId=2332565944
		原有gc配置
		    -XX:+UseG1GC
		    -XX:+UnlockExperimentalVMOptions 解锁实验性 JVM 选项	允许使用实验性 GC 相关参数
		    -XX:MaxGCPauseMillis=70 目标最大 GC 暂停时间 70ms	G1 会调整回收策略，尽量满足这个暂停时间目标
		    -XX:G1NewSizePercent=35 新生代最小占比 35%	控制新生代初始大小，提高 YGC（年轻代 GC）吞吐量
		    -XX:G1MaxNewSizePercent=45 新生代最大占比 45%	限制新生代最大大小，防止老年代被挤占
		    -XX:G1MixedGCLiveThresholdPercent=85 老年代对象存活率高于 85% 时不参与 Mixed GC	避免回收高存活率的区域，提高 GC 效率
		    -XX:G1HeapWastePercent=5 允许 5% 的堆内存浪费	让 G1 保留一定的可用空间，提高回收灵活性
		    -XX:InitiatingHeapOccupancyPercent=35 堆占用率超过 35% 时启动并发 GC	让 GC 提前介入，减少 Full GC 可能性
		    -XX:+UseStringDeduplication 开启字符串去重	降低重复字符串的内存占用，减少堆压力
		    -XX:+ExplicitGCInvokesConcurrent System.gc() 触发并发 GC 而非 Full GC	避免 System.gc() 造成长时间 STW // todo
		    -XX:+ParallelRefProcEnabled 并行处理引用	加快 GC 处理弱引用（如 SoftReference、WeakReference）
		    -XX:MaxMetaspaceSize=368m 设置 Metaspace 最大大小 368MB	限制类元数据占用空间，防止 Metaspace OOM
		    -XX:MetaspaceSize=368m 设置 Metaspace 初始大小 368MB	预留一定的类元数据空间，减少扩容时的开销
		痛点 服务偶发性STW时间达到秒级 ccweb 单次gc时长长达800ms，每分钟gc时长（疏散停顿：用于对象拷贝）长达近10s （实际4~5s）每分钟平均6~8次gc
		必要性 
			g1的缺点：
				老年代空间碎片严重，触发 Full GC
				预测失败，GC 来不及回收，YGC、Mixed GC 频繁退化为 Full GC
				回收效果差，老年代释放空间少，下一次 GC 更快触发 → GC 风暴
			面向C端服务不能容忍高延迟
		方案选型 jdk17 zgc jdk21 分代zgc，为了项目稳定性以及兼容现有中间件（spring kafka es等）版本选择了jdk17
		难点 升级需要适配各个中间件的版本，项目稳定性需要保证
		解决 从小型项目开始逐步升级，不同版本灰度发布
		    -XX:+UseZGC
		    -XX:ConcGCThreads="$con_gc_threads" 并发的线程数，越少越好，但是会影响并发阶段的耗时
		    -XX:ParallelGCThreads="$par_gc_threads" STW的线程数，越多越好，降低暂停时间
		    -XX:ZCollectionInterval="$zgc_interval"
		效果 
			偶发性的长时间STW消失，STW时间缩短到微妙级 ccweb 单次gc最大1ms，每分钟gc时长nms，每分钟30次gc左右
			每分钟gc次数 30次->个位数7,8次
			每分钟gc时长 4s-40s -> xxms-xxxms
			最大gc时长 40s -> 600ms

	jit优化（服务启动时cpu占用100%触发cpu限流从而影响其他各个关键指标 问题定位解决）
		https://confluence.shopee.io/pages/viewpage.action?pageId=2594282396
		问题定位
			代码问题 （规范代码，ci时设置准入门槛）
				数据预加载（配置文件：白名单等）
				内存缓存大量protobuf对象
				tomcat启动线程数过多
				打印log中请求body也打印
			配置问题 （升级配置，c2延迟编译 限制c2编译线程数）
				4c8g -> 8c16g
				jit在服务启动时，因为请求过多+c2编译器编译热点代码导致cpu占用多了25%左右
		其他方案
			服务拆分（治标不治本）
			服务扩容（浪费资源，日常的cpu使用率20%左右）



	hbase
		hbase版本 xx
		content_supply__ls_streamer_item_offline 每月增长 1.5t左右 现在19.6t 有time_dim字段，分1d 7d 30d
		content_supply__ls_session_minute_offline 每月增长 0.3t 现在3.8t
		除此以外还有mcn的item数据，分钟直播数据，短视频数据，短视频商品数据
		主要是一些持续增长的大批量的存量数据，存mysql不易拓展，业务场景也不需要复杂的查询条件
		row_key设计 基本都是根据业务场景的查询范围确定，比如按streamer维度就是streamer_id % 100 _ streamer_id _
		线上问题
			scan操作太多，cpu干限流了
			解决
				scan snapshot
				合并scan操作
				降频
			大量bulk load触发compaction 争抢正常IO
			解决
				调整bulk load策略，降频限制线程数


	mysql问题
		版本 5.7
		集群配置 一主两从 已经从phy迁移到rds
		监控团队集群维度+服务维度
		线上负载
			整体db综合读写比 2 ： 1 整体OPS在 70k ~ 160k左右 （都是主库）
			qps 主从比不同集群有的是 2： 1 有的是1 ： 1
			以下都是服务的容器配置
			cc web 1k qps 8c 16g 20i
			cc job 1w qps 16c 32g 22i
			ls job 6~7w qps 24c 32g 58i
			id supply_db_000000xx 80g * 16 qps 
			id ls_db_000000xx 500g * 16 每个月25g增长 qps 主 2.8k 从 1.8k * 2 dr 2k buffer pool 50g左右 直播overview大概60mb * 100 * 16 minute数据 大概1.5g * 100 * 16（每月清理）
			vn ls_db_000000xx 400g * 8


		线上配置
		慢查询排查
			有一些慢查询超过1s，不影响正常业务，被认为是mysql的正常抖动

		线上问题
		可用性
			读写分离，不需要最新数据的读取都走从库
			索引优化、分库分表

	redis
		codis
		版本3.2
		主从配置 1从 未启用cluster 用sentinel进行故障转移
		allkeys lru策略
		主从lag 1s同步
		为什么不用集群？
		只使用 RDB（快照），AOF 没开启。
		但 RDB 似乎是手动触发，没有自动执行 save 规则。
		如果 Redis 崩溃，可能会丢失最近的变更，因为 RDB 只在上次 bgsave 之后保存了数据？

		数据
		一些实时数据包括

		codis vs cachecloud (封装了redis，提供各种管理监控功能的一款产品)
		Codis	3.2.6	基于redis原生，主从模式，用哨兵sentinel做故障转移	支持多db	社区已不活跃，redis版本多年无变化
		Cachecloud	6.2.6	基于redis-cluster原生实现	只支持db0	自研产品，有专门的研发团队，支持较好
		codis迁移到cachecloud
		迁移策略

		平台迁移：大部分集群可以通过平台自动迁移，减少业务侧的变更需求。
		业务改造迁移：特定情况（如多 DB 集群和无密码访问集群）需要业务侧介入和代码改动。
		迁移步骤

		创建 CacheCloud 集群：与原 Codis 集群配置相同，保证平滑切换。
		数据同步与一致性检查：确保数据在迁移过程中的完整性和一致性。
		域名与流量切换：更新 DNS 记录和 HAProxy 配置，切换流量到新集群。 // todo 平滑切换
		关闭旧集群与数据同步：完成迁移后关闭 Codis 集群和数据同步任务，确保新集群的独立运行。
		回滚计划

		准备充分的回滚策略，包括恢复旧的 DNS 设置和重启原 Codis 数据同步，确保在遇到不可预见问题时可以快速恢复服务。

		copi2 vs kv store
		自研的kv存储，兼容redis语法，基于raft进行节点同步

		线上问题
		大key问题？ 如果一个key的大小为1MB，每秒访问量为1000，那么每秒会产生1000MB的流量。这对于普通千兆网卡的服务器来说是灾难性的。
		规范数据类型，少用list只push的操作，设置过期时间，用del删除大key，但是4.0之前是O(n)复杂度会阻塞进程，需要写脚本分批删除
		大key监控+限流 缓存预热 本地缓存兜底
		缓存大促时有压力吗？如何应对
		数据量怎么样？未来如何拓展？ 
		通过Prometheus埋点监控热点Key，通过Prometheus查询接口精准过滤查询并定时更新进入本地内存，减轻redis负担，相比于全量缓存本地更加节省内存。


	es:
		版本
			7.8.1
		存储容量
			主+副本 7t左右 空间占用1年增长了1.5t左右 集群总共220T,显示磁盘占用了33T，可能是因为删除数据（docs.deleted 显示你的索引有 超过 7.9B（79 亿）条已删除的文档）以及其他占用
		参数调优
			主：副本=1:1
			异步落盘 5s异步刷translog
			禁止使用dynamic mapping，设置为strict，避免字段数量过多
			index_options没优化，因为好像所有字段都需要检索
		应用场景
			直播商品数据 直播下的商品列表排序 直播下的商品各个维度的分钟级窗口数据 （利用es能够对nested的某个子字段sum后直接排序的特性）
			直播数据 内部看板 直播列表需要各个维度筛选
			主播数据 内部看板 主播列表各个维度筛选
		性能水平
			到shard的请求分布均衡，没有热点shard
			数据分布均衡，33个分片，每个分片大小一致
			读写压力都不低，但以 Get Rate 为主（某个索引读取量特别高）。id地区商品分钟数大约为20k/s，其他地区像vn只有2k/s
			CPU 负载适中（最高 69%，平均 24%~48%）。
			Query 延迟（查询延迟）普遍较低，但偶尔有波动。一般是个位数ms级，偶尔波动到1s左右
			Refresh Rate（刷新频率） 也比较均匀，说明 ES 的 refresh_interval 设置合理，没有导致过多的索引开销。
		备选方案
			es
				索引极其占用空间
				写放大严重 需要merge segment
			ck 不适用于搜索场景
				不适用于高频更新 数据吞吐量大的时候，底层mergetree会很慢
			mongo
				搜索功能，聚合功能都不太行
		线上问题
			为了应对双12大流量（大约平时峰值的1.5倍）
		性能优化
			深度分页search after讲一讲
			异步落盘 5s异步刷translog
			批插批写 一次拉取1600条，24并发处理 get rate 380k/s index rate 261k/s
			模型优化
				只需要聚合不需要搜索，index设成false
				不需要算分，Norms设成false
				禁止使用dynamic mapping，避免字段数量过多
				优化index_options，控制在创建索引时哪些字段会添加到倒排索引
				根据业务需求，选择是否关闭_source 由于需要支持update, 我们这里不能关闭_source
			路由
				使用ES默认路由: 按文档id路由, 文档id : ${sessionId}-${itemId}
				确保ES分片均匀的分布在各个物理节点上
				设置合理的分片数
			cpu和io
				减少不必须的分词
				避免不需要的doc_values
				提高文档压缩率-文档写入时保证字段顺序
				提高文档压缩率-当字段值不变或字段为0时可以不写入 
				ES配置- refresh interval（牺牲搜索实时性） 5s刷新
				ES配置- translog async（牺牲可靠性）勾选为true
				根据业务需求, 选择是否使用ES自动生成的文档id 由于需要严格保证每个直播间-商品只能有1条数据，因此这里选择手动创建文档id: ${sessionId}-${itemId}
		流量压测
			上线前在liveish环境创建测试索引，引流live环境消息无积压消费12小时，对比es文档，观察消费情况
			压测时，选择业务低谷期，降低对业务的影响
			验证时，通过脚本及现成api，对正式索引和测试索引的数据进行批量验证，确保新consumer和旧consumer业务逻辑一致
			上线时，由于商品消息消费是幂等的，因此通过先双写(新旧consumer同时写正式索引)->验证->关闭旧consumer方式来实现平滑切换。在业务低谷期按地区挨个切换，优先切换流量小的地区
		拓展计划
			已删除文档过多，准备定期force merge segment
			进一步优化index_options 控制不必要的倒排索


	kafka
		3/3为例
		session流量 3k - 1.5w
		商品 1w - 7w
		订单 100 - 3k
		总流量 4w - 14w
		latency de生成/发送延迟都是十几秒
		be消费也是十几秒（除no kol mysql有限流以外）

		线上问题：
		一次性拉取过多，导致消费速度过慢超时，超过max.poll.interval.ms发生rebalance，重新跟broker建立链接之后，又从原先的offset开始消费
		相同broker，不同topic，group id设置相同,但是topic鉴权不相同，导致rebalance之后无法消费，消息堆积


	数据指标
		db占用
		es占用
		kafka

	性能优化
		https://confluence.shopee.io/pages/viewpage.action?pageId=2284141477
		直播大屏es：https://confluence.shopee.io/pages/viewpage.action?pageId=2153729220



特征平台

	数据源 -> 数据 -> 整合处理 -> kv(用作特征匹配判定根据userId查相应特征 userId - 多特征key -> value)    通过特征圈选人群包、传入userId判定特征
					-> es（多特征到userId的映射） 



	用户特征  来源 应用场景



	资源申请
		copi2：	key个数 7000w
				key类型 hash 
				key长度 20byte
				字段数 200
				字段长度 10byte
				value长度 10byte
				总计：500G

		运行模式：高性能/大存储
		预估总容量（key+value+buffer）1000G
		预估QPS：20000 = 1000（下游服务qps） * 8（下游服务个数） read 120（特征接入任务数） * 100（单个任务qps） write
		使用命令：hget hgetall hset
		读/写命令分布占比：40% 60%
		最大value长度 200 * 3byte
		平均value长度 10byte
		key总数量 7000w
		平均key长度 10byte
		平均TTL -1
		客户端并发连接数 100
		预期读命令P99 10ms
		预期写命令P99 10ms
		是否有plan B 有
		能否容忍机房故障 不能


		es: 单文档大小 =  200 * 4byte(普通数字) + 20 * 50字符 * 3byte（utf8字符串） = 3800byte / 3.8kB
			索引数 = 2
			文档总数 = 2000w(用户特征) + 5000w(视频特征)
			源数据大小 = 3.8kB * 7000w = 266GB 
			总容量大小 = 源数据大小 * (1 + 副本数量) * (1 + 索引开销) / （1 - 操作系统预留）/ (1 - 内部任务开销) * (1 + 预留空间) 
					 = 266GB * (1 + 1) * (1 + 0.1) / (1 - 0.05) / (1 - 0.2) * (1 + 0.5)
					 = 266GB * 4.35
					 = 1.2TB


		版本号：7.8.1
		业务场景描述：存用户/视频特征数据
		预估容量（总容量，包含副本）： 1.2TB （600G + 副本）
		副本数：1
		索引数量：2（用户表 + 视频表）
		文档数量：7000W = 2000w(用户总数) + 5000w(视频总数)
		index rate（写入速率，docs/s，index rate=写入文档数*副本数）：100（单个任务docs/s） * 100(任务数) * 1（副本数） = 10000
		单文档平均大小（kb）：3.8kb
		search rate（查询速率，search rate=查询qps*单次qps所查询所有索引的主分片之和，自定义路由除外）：100（qps） * 200（分片数） = 20000
		查询类型（term match，agg....等等）：term match
		整个集群总的分片数：2索引*100分片每个索引=200
		PS：1.es索引设计注意点：1.单分片容量尽量不要超过20G，2.单分片doc条数硬上限20亿条
		        2.各种复杂的分词逻辑尽量不要用es去完成，比较损耗es性能，尤其是ngram分词器或者一些第三方不可控的分词器。

		ES和CK都可以做人群圈选，但各自优势不同：

		CK更适合多维条件组合、高并发、海量数据实时聚合场景。典型如实时用户画像、精准营销、实时推荐、复杂圈选条件。
		ES更适合文本搜索、模糊匹配、检索类需求场景，如日志全文搜索、商品检索、关键词模糊匹配等场景。
		如果纯粹做人群圈选（尤其复杂圈选），CK优势明显，实际生产环境下企业更普遍采用ClickHouse来实现。

		除了人群圈选还有模糊搜索的功能，其次较多为离线数据，性能要求不高

		Redis：

		改进 key 结构：减少 hgetall 依赖，优化 hash 结构，提高 pipeline 传输效率。
		优化存储：调整 hash-max-ziplist 配置，避免 hash 过大影响查询效率。
		高可用方案：双机房 Cluster 或 Proxy 方案，避免单点故障。
		ES：

		调整分片策略：如果 term match 查询居多，routing 到 userId 可能减少分片查询数。
		优化索引：doc_values 关闭不常用字段，refresh_interval 增加至 30s 降低 merge 负担。
		冷热数据策略：减少 hot 索引查询压力，提高 search 响应时间。
			Elasticsearch 自动根据时间动态迁移数据的核心是 Index Lifecycle Management (ILM)，它可以按时间自动将数据从 Hot 节点迁移到 Warm/Cold 节点，并最终删除老数据（也可以不删除）
		ClickHouse 作为 Plan B

		如果后续圈选需求变复杂（如 实时计算 场景），ClickHouse 可作为优化方案。



有道少儿英语订单系统拆分


项目介绍

这个订单异步化改造主要是对我们订单原有基于线程池的方案就行了一些优化，因为之前我们订单下单后的一些逻辑，包括开课、短信、活动、物流等，都是在用户支付完成后，在线程池中进行的。

这样就可能会导致一些问题：

首先是当订单量如果突增的话，大量订单会导致线程池拒绝的风险，
二是之前的过程中，对于业务执行失败的订单，缺少了异常记录的操作，
三是所有不同类型的业务耦合性太强，希望去对他进行一个拆分，比如说一笔订单支付成功后，活动的执行失败不应该影响后续开课的业务执行。

因此对他进行了优化，当时采用了canal+kafka的方式，我们这边订单支付后的回调接口，是将订单状态由0改为1，所以canal只需要监听订单表的变化订单；发送到kafka中，对于这个topic我创建了一个负责清洗的消费者，会解析canal发送的消息，只过滤状态值从0变为1的订单数据；

之后将订单id再次进行转发；

另外我是将订单后置的履约业务拆分为独立的互不影响的几个子业务，将一些共有的方法抽象为一个抽象类，其中提供了模板方法，每个子业务只需要继承这个抽象类，在其中定义自己的业务码，编写各自的业务逻辑就可以很好的扩展，也方便了以后其他同事的开发。

每个子业务都作为一个消费者组来对支付成功的订单topic进行监听，接受到以后通过orderId就可以还原订单、商品、用户的一些数据信息，通过在redis分布式锁+mongo中记录执行结果，来确保原子性避免重复消费（因为对每个子业务定义了业务码，可以区分）。其次对于执行异常的订单，会根据订单id、业务码记录下来，通过定时任务补偿的方式确保最终一致性。


可能后续会问道或者可以在介绍中补充的问题：

线程池相关，各个参数和提交流程
canal的机制，mysql数据同步
一致性方案
分布式锁
订单量特别大怎么办：读写分离、缓存、分库分表（https://blog.csdn.net/weixin_48182198/article/details/108475822、）


订单系统
	订单流程 下单：不同入口（h5，公众号，app，小程序）->创建订单->支付：不同渠道（支付宝、微信、苹果）->支付回调->履约（开课、发短信、发消息、优惠劵、物流）
	订单信息 商品信息 + 用户信息 + 支付信息 + 履约信息 + 物流信息
	订单状态 初始状态->支付->（支付成功、失败、超时）->（开课中、成功、失败）->（退款中、成功、失败）
	订单实际方案 基于base法则 尽量确保最终一致性 线程池+定时任务xxljob
	订单存在问题 线程池问题：订单量激增导致线程池拒绝、业务没有隔离开、大量线程池容易导致fullgc、没有补偿机制和监控 定时任务问题：@schedule单线程导致任务饿死、任务没有设置优先级、没有监控和报警机制 其他问题：统计类需求和用户 
	重构方案 线程池->kafka消息队列 重试补偿机制 定时任务->xxljob 任务增加逆向操作流程
	遇到问题
	分布式解决方案
	canal
	kafka
	订单数据怎么过滤的？kafka的生产者从哪来？ canal server配置来的
	实际kafka的配置？ 6 partition 有没主从 不知道
	canal的角色 监听的是库还是表 mysql的整个bin log 但是可以过 滤发出的消息 表级别过滤 如何保证有序 消息没有强关联不需要有序，而且每个partition中的消息也是顺序的 消息格式是怎样的 一条消息多大
	kafka 配置如何 多少个topic 一个topic，后续履约流程每个流程一个消费者组，消息订阅发布模式 多少个partition 反正>=3 如何保证接受的消息有序 消费者concurrency和partition保持一致
	消费出现问题如何重试，应该怎么办 首先kafka不支持重试 其次应该保证消费的幂等性，接口层面记录执行日志，以及加上分布式锁
	消息消费出现了并发问题如何解决
			redis分布式锁
	订单系统如何设计 
	表设计问题 订单id 商品快照 其他信息
	状态流转问题 支付 开课 退款
	事务一致性问题 spring事务 单库事务 后续服务拆分就需要分布式事务 seata/ JTA
	数量大了怎么办 分库分表 
	

如何保证代码质量
	统一规范 阿里java规范
	覆盖单元测试 重要接口跟工具类有对应的单测用例
	接口/集成测试（自动化流程） qa在test环境有每日的自动化测试用例
	静态代码扫描 sonarlint扫描代码，未满足条件不允许提交
	code review代码评审机制 merge到release前需要核心成员review代码
	分支管理清晰 不同环境的分支命名规范，git使用规范

大促前的准备
	核心链路梳理 核心链路为直播数据展示链路
	容量评估/压测 评估大促期间直播数据的峰值以及写入量，用上个月的峰值 * 1.5/2 压测都是手动
	缓存预热、热点数据隔离 因为db跟kv不是cache aside的关系，而是异构存储，所以不需要缓存预热，db倒是可以把kol的数据预加载到buffer pool中，es的数据可以通过提前索引，force merge降低存储，提升性能
	限流降级策略 多数据源主链路为es/kv 备用链路为mysql kol数据链路可以降级，确保kol的直播看板服务可用
	灰度发布机制 大促期间封板，如果非有新功能在大促期间发布，可以在某些容器上进行灰度发布，怎么灰度？前端跟后端如何灰度同步？通过ab团队配置，前端不同用户拉取不同版本显示不同样式，后端可以走金丝雀发布（如果需要观察新服务是否稳定，比如写链路优化改批插批写）
	告警 监控升级 建立包括某些业务指标（接口）跟技术中间件指标（es，kafka等）的告警看板，admin平台上可以查看某一天产生的直播数据以及转化率的业务指标
	应急预案准备 大促前根据压测结果，对某些关键功能进行升级（代码升级：优化逻辑，配置升级：加机器）制定应急预案，确定线上oncall的责任人
	异地双活 没做，因为我们是根据区域定制化配置的，新加坡机房跟美东机房的数据做了隔离


跨团队沟通
	换位思考 了解对方的难点，陈述项目的重要性
	对齐目标 站在整体业务的角度去推进项目
	借助机制 通过项目会推进，拉齐leader推动


待补充的部分
	
	服务实例动态扩缩容 
		pod层面根据cpu io动态拓展
		node层面当pod资源不足时动态拓展


自身优势/成长
	跨团队协作：推动协同落地、保障交付质量
		拉通对齐，识别风险点，跟其他团队对齐字段口径，梳理业务指标，
	推动规范：有 owner 意识，重视系统长远可维护性
		输出技术规范，推广代码准入基准
	带人经验：注重代码质量与工程规范落地
		code review，帮助同事梳理项目背景，清理开发边界
	技术影响力：在高并发/海量数据系统中持续输出优化方案
		输出技术文档，做技术分享，分享调优等经验


离职原因
	希望系统更有挑战
	更加贴近用户
	希望项目更多闭环
	能够为公司创造收益，实现自我价值


职业规划
	希望通过自己的经验给公司带来收益，从技术出发，证明自身价值


----------------------------------------
坑爹问题
----------------------------------------

问题 1：你的职级是多少？有晋升吗？
背后目的：
想确认你现在在哪个层次；
想知道你是不是在原公司已经“封顶”，或者“长期没晋升”；
想看你对职级和成长的态度是否健康。
标准答法模版（真实+略包装）：
“我现在职级是 P6，在我们这边属于中高级工程师。过去两年整体对技术深度和项目推进能力有较大提升，也开始参与一些架构设计和跨团队协作。
但我们公司晋升节奏偏慢，整体机会比较有限，P7 卡得比较紧。所以我也想看看外部有没有匹配成长路径的新平台。”
关键词：
把自己定位为「能力在往上走」，但「环境限制了成长速度」；
不显得你是晋升失败，而是晋升路径有限；
让对方理解你不是混资历，而是有成长意愿。


问题 3：你之前投了我们其他岗位/公司，为什么没推进？你知道什么原因吗？
面试官想判断：
你是否被别的团队否定；
你是否面试表现差、但自己没意识；
你是否有「反思能力」——而不是一问三不知。
最忌回答：
“我也不知道，也没告诉我。”
这等于告诉面试官：我没复盘、不够在意、被拒也不思考。直接扣印象分。
最优答法（真实 + 反思）：
“我之前确实投过贵公司另一个方向（或岗位），当时反馈是技术深度还可以，但可能在表达和架构思路梳理方面不够清晰。我后来也复盘了下，准备过程中确实偏重细节，没在宏观结构上提前讲清楚项目背景和拆解思路。
所以这次我也特别注意了答题结构和背景清晰度，想更完整地展现我在系统设计和业务理解上的思路。”
这种回答是黄金级的：
你不否认被拒；
你不躲避原因，还主动总结；
你展现出成长、反思和进步能力。


你跟上一轮面试官聊的怎么样
一面我觉得聊得非常顺利，那个岗位整体业务目标是xxx，技术核心是xxx。我们还讨论了一些具体场景，比如xxx。
我觉得比较匹配的点是：我之前做过类似的高并发链路 + 数据分发场景，比如直播大屏、特征平台这些系统，在处理指标一致性、异步解耦、系统稳定性上有较强经验。
所以从业务目标到技术挑战，我都挺感兴趣。


你是通过跳槽来晋升的吗
确实，我目前的几次职级提升，主要集中在换平台的时候实现的。
但我不是那种盲目跳槽涨 title 的人，每次跳槽背后，其实都是围绕着我核心成长目标去推进的。
比如早期我在做直播大屏系统，当时积累了高并发、实时流量处理这块的经验。后来我意识到单纯做展示层对我的工程成长是有边界的，所以我选择去了能更贴近数据中台和稳定性链路的方向，像特征平台、订单异步处理链路这类系统，补了我在数据分发、异步解耦、幂等保障、链路容错方面的深度。
再往后，我越来越关注系统之外的业务落地问题，比如怎么让系统真正推动业务增长和变现，怎么围绕用户行为去设计激励策略，这也是我现在选择商业化方向、剪映团队这边的原因：我希望能把之前的系统能力和稳定性经验，真正和业务目标结合，往产出导向走一层。
所以对我来说，职级并不只是 title，而是某个阶段能力和视角成熟之后的自然体现。未来我也希望通过真实业务项目的成果、架构优化的沉淀，在一个更长期的节奏里自然推动成长，而不是每一步都靠换组织。

给不到你2-2接受吗
我理解职级评定是综合匹配岗位要求、业务挑战、团队定位的评估结果，2-2 是我期望的目标段位，因为它也比较贴合我目前的系统经验和业务承接能力。
但我其实更关注的是是否有成长空间 + 是否能打出成果。如果短期定位是 2-1，只要职责清晰、产出空间足够，我也愿意在落地之后用结果去推动成长路径。
对我来说，更关键的是这个方向是否能让我把之前系统能力 + 数据理解更好地结合到业务目标中去，只要这个方向没问题，具体职级我这边是有弹性的。


时间不够 需求不稳 没时间plan B
如果节奏已经定死，我会做这三步：
第一步，切 scope
和业务方/相关 owner 明确【核心闭环目标】是什么，比如哪条链路必须打通，哪类数据必须埋上。
所有非主流程的监控、冗余能力、优化部分都先砍，等主链路稳定后再补。
第二步，拉人打补丁，但结构留口子
技术方案上我会压简版本，但把扩展口提前留好，不做死方案；
比如我曾在某项目中用临时 topic 方案上线，但把 route 模块设计成可插拔，方便后续替换成正式链路。
第三步，自己亲自兜底 + 后续修复日程先排好
我会上线前写好 owner list 和紧急预案，谁出问题谁打，防止值班混乱；
同时我会跟团队同步【上线后哪一天就要补哪块】，不给团队留下烂尾口。


跨团队协作，对方不配合、不回、不对齐资源，怎么办
第一步：找协作的“阻塞点”到底在哪
是时间冲突？优先级不一致？对方不了解背景？我会先问清楚再做判断。比如之前我推进一个活动数据链路建设时，数据组不给支持，后来发现他们以为这只是我们私自加需求，我就补了一份背景文档+正式需求单再提一次。

第二步：通过中性角色/机制解决协同偏差
我会找 PM 或项目 owner 一起拉个协调会/对齐文档，不是为了 PUA 对方，而是明确边界：你要承担什么，我这边来兜哪些。对方如果有资源卡，我也会看看有没有替代接入方式。

第三步：有分歧就同步双方 leader，不私下扯皮
我不会直接 escalate，而是同步我们 leader和对方 TL/Owner，“我们卡在哪了，当前我们能承接这部分，对方是否可以协调一下 X 时间或资源”，让组织级别的人知情，有底线有尊重。

第四步：如果资源始终协调不动，我会做组内兜底 + 留解耦设计
我曾经自己组里硬上一个消费模块 + 自定义 ETL 分流逻辑，上线后再慢慢谈资源，整个链路至少能跑，不影响主目标。
兜底是态度，解耦是为了未来不被捆住。


----------------------
团队协作项目落地
----------------------


一、团队协作类
1. 遇到团队成员不配合或者进度落后，你怎么处理？
S：在一次直播大屏项目中，后端接口需要和另一个小组对接，对方开发初期配合不积极，导致接口进度落后。
T：我需要推动双方协作，保证项目按期上线。
A：我先主动了解对方的实际困难，发现他们并不是故意拖延，而是临时被拉去支援另一个项目。我协调了排期，把我们依赖的接口划分为「必要上线」和「可延期优化」两类，并临时兜底一部分逻辑。
R：最终我们在上线前完成了关键路径，非关键部分也在后续一周迭代补齐，整个项目未受影响。这个事情之后我们也建立了固定的跨组周会机制，合作顺畅了很多。

2. 跨部门合作对方负责人态度消极，你怎么办？
S：做数据分析平台接入时，需要内容团队配合提供内容标签映射规则，但对方业务负责人表示“我们现在没有资源支持”。
T：必须拿到这些规则才能保障标签准确性，否则影响整体数据质量。
A：我先从使用结果出发，做了一次数据回溯对比，指出当前标签准确率低下对其业务带来的影响；同时拉上我们部门 Leader 去同步资源评估和时间线。
R：对方同意安排一个实习生支援我们做初版，我们也主动分担了部分编码工作，最终把标签规则方案在三周内落地并在月报中展示了明显提升效果。

3. 你有 mentor 新人的经历吗？怎么做的？
S：我曾负责带一个刚入职的小伙伴参与直播商品模块的开发。
T：要在保证交付进度的同时，让新人尽快熟悉代码、流程、业务。
A：我一开始把任务拆分为几个层次（改 bug、小功能、主流程），每个阶段带他过需求、代码结构、上线流程。遇到他不理解的地方，我会让他先尝试，再引导他自己调试找答案。
R：他在 2 周内就能独立完成接口开发，1 个月后可以独立承担小模块开发，我也从中总结了一份带新人文档供团队后续使用。

4. 你在团队中是如何提升效率或协作的？
S：我们团队每次临近大促时，排查接口日志耗时和告警效率都很低，大家各自查各自的日志，效率不高。
T：我想统一日志结构、告警规范，提升效率。
A：我发起并主导了统一日志格式的方案，引入 traceId 链路追踪，和 SRE 合作建立实时告警策略，并写了一份统一排查 SOP 文档。
R：大促期间问题排查时间从平均 20 分钟降低到 5 分钟，团队其他人也反馈效率提升明显，后来这套方案推广到其他直播子服务团队使用。

二、项目落地与高压应对类
5. 项目临上线突然出问题，你是怎么处理的？
S：直播大屏在上线前 1 小时发现接口 QPS 激增时延暴涨，触发预警。
T：紧急找出原因、兜底防止宕机。
A：快速对比了压测与线上环境差异，发现某 Redis 缓存设置了错误的 TTL，导致每次都回源数据库。我立刻更新配置并热更新，同时联系 DBA 临时提升了该表的索引缓冲。
R：接口延时快速恢复在阈值内，上线未受影响，后续也写了预检查脚本提前校验线上配置。

6. 做过的最有挑战的项目是什么？怎么解决的？
S：直播商品榜单模块，要求在 10K WPS 的订单写入流下，实时更新每分钟榜单，并支持多维排序（GMV、订单量、点击量等）。
T：需要保证计算准确、延迟低于 1 分钟，同时系统可扩展。
A：我主导设计了 Kafka + Redis + 异步排序计算方案，关键数据放 Redis 热 key，并定时落 HBase；排序部分引入分桶和局部 TopN 算法降低内存压力。
R：最终上线系统延迟维持在 20~30 秒内，支撑高峰期百万商品实时排名，成为我们团队重点展示成果之一。

7. 需求不断变化、时间紧，你如何保证交付质量？
S：一次临时活动类项目，运营频繁改需求，需求文档一改再改。
T：要控制风险，不能无限被牵着走。
A：我首先推行了“需求冻结点”，即临近上线前三天只允许改文案 UI，不允许改核心逻辑；同时我把需求变更影响拆解为「核心影响」「轻微影响」，设置对应回滚方案。
R：项目最终顺利上线，即使上线前还改了一次入口逻辑，也因为有灰度和回滚做了兜底，没有影响主流程。

8. 没有产品或业务支持的情况下推动技术改进，怎么说服？
S：我希望推动我们服务上线前接入链路压测体系，但初期产品觉得没必要，认为“压测只是 SRE 的事”。
T：我要说服业务和产品理解压测的重要性并同意投入人力做接入。
A：我整理了过往 3 次大促事故中压测缺失的代价，用图表展示如果提前压测可以避免的情况；并手动做了小规模压测 demo 给他们看效果。
R：产品最终认可重要性，同意排期接入。我们在后来的 618 和双 11 都提前做了压测，暴露了几个潜在问题，避免了事故。

三、面试官可能跟进的提问简答
你如何在没有话语权的环境下争取资源？
→ 用影响力而非职位权力，展示问题严重性、对业务的影响，用数据和案例赢得支持。

你怎么衡量一个项目是否成功？
→ 除了按时交付，还要看用户反馈、系统稳定性、是否提升了业务关键指标（如 CTR、GMV），有没有复用价值。

你做了哪些事让本来快要失败的项目最终成功上线？
→ 主动梳理阻塞点，设立 check list，推进多方协作，制定灰度兜底策略，在压力下维持主线不掉。







--------------------


好了，我来回顾一下字节三面的问题，然后你帮我看看为什么面试官觉得我技术深度不够

首先是开场自我介绍
然后面试官问我项目，然后我说了下直播大屏的技术难点之类的
高并发
大流量吞吐
实时性
然后我说通过多组件异构存储kv+es+mysql以及kafka多partition支持横向拓展
然后他问
kafka本身是通过什么方式实现的高效
我说类似于epoll之类的，共享内存+send file减少io时上下文切换成本
他又问这是同步还是异步io，我想了下说这是异步，然后io分阻塞非阻塞，然后举了个等烧开水倒水的例子，他表示没听懂
然后他又让我做题
实现随机取set元素
我刚开始想能不能链表+hashmap实现o(1)然后，然后他提示用数组+hashmap
写完之后
他说有些边界问题
我修完了
他就问代码里class非static的变量跟引用是存在堆还是占哪的
我想了下犹豫地说堆吧
然后他问怎么保证线程安全
我说synchronized，他说成本大，我想了下说对单个变量加锁，他说这能保证整体操作线程安全吗，我说不能，那只能用synchronized了，他说好吧
然后他又问了些其他问题
我看你之前都是处理数据的吗，可能因为我简历上都没有提现业务项目，我说有业务，aigc之类的，55开吧
然后他又问你觉得aigc最复杂的点是什么，我说是任务的流转以及跟其他业务团队之间跨团队合作
然后就是：
你觉得国内试穿的大模型做的最好的是什么模型 我说淘宝的
你看机会的原因是什么 我说业务场景，职业发展，薪酬职级
只给你2-1你接受吗 我说看面试情况
你觉得自己最大的优势是什么 我说提效，通过自发研究特征平台给团队提效
你有什么要问我的 我说团队在ai这块提效又做什么努力吗
然后没了


-----------

他问了
你职级多少，对标p6吗 对
你之前是靠跳槽来涨薪吗 对
java跟go的区别，什么地方用go什么地方用java java偏商业化 go偏高并发场景
他打断）我想让你讲下io密集型 计算密集型之类的 然后我说内核态切换巴拉巴拉

说一说你项目里有哪些可拓展性的点
我说代码模板方法快速接入
笑着疯狂打断）
然后他说这p6都知道，我们这个岗位是e9对标p7的，你上来就讲这个我没了解项目背景
你准备下再回答（我想这不是你要关键点吗，顺着你有不满意
然后我整理了几分钟
稳定性
数据上游 数据校验 数据监控 核对
传输过程 dr 跨机房容灾
消费过程 降级 熔断 不同用户分链路
可拓展性
消费层面可拓展
数据层面可拓展
地区层面可拓展
回答完
他说这才是他想要的

然后


你觉得评价一个人的标准是什么 我说各个方面，技术水平，能力，合作态度，但是主要看结果吧，结果导向

然后问我什么要问他的

我问了第一团队现在的痛点
第二你愿意给哪些人好绩效

他回答

自动化运营
服务创作者
分析发放线上化
机器学习模型给出建议
运营体系基建 洞察

认知 价值 方法 
技术能力 架构能力 技术执行
管理能力 管理效率 1+1》2







