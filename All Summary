
自我介绍：
	面试官你好，我叫吕超毅，17年毕业于华中科技大学软件工程学院，现在在shopee video supply团队负责Java开发

	我们的团队职责主要有两块：
	业务层面
		video的创作者相关，包括：创作工具、内容变现、流量激励、任务相关、主播助手
		live相关，面相卖家的内容，包括：直播大屏、直播诊断、AIGC
		内容供给相关，包括：短视频图片的爬取
	技术层面
		数据中台的职责，提供数据服务，包括主播端的实时看板、以及面向运营的一些离线数据的统计和可视化平台

	我在团队中的职责也分两块：
	业务层面
		日常业务owner项目以及代码开发
			技术选型
			输出技术文档
			资源申请
			风险评估
			技术分享
	技术层面
		项目的维护以及架构优化
			日常指标监控
			大促期间的问题定位
			系统的稳定性优化
			告警监控问题处理

	业务团队介绍
		团队职责
			创作者内容供给 主播流量激励、任务体系、创作周报
			卖家内容供给 直播实时大屏 
			视频图片内容供给 商品图爬取，aigc图片产出
			数据中台 实时数据、离线数据 聚合看板
		数据范围
			直播数据
			视频数据
			商品数据
			创作者数据
		


	之前主要参与过的项目有：直播大屏系统（基本的业务代码， 日常的报警监控，es的优化，分库分表）、特征平台（负责架构设计，项目搭建，风险评估），有道少儿英语的订单系统（主要负责订单系统的重构和优化）

	主要的技术栈有：Java全家桶（包括Spring、SpringBoot之类的），数据库包括：MySQL、Redis、MongoDB等，其他中间件的话：kafka、ElasticSearch用过一些


重点项目介绍：
	
	架构图/项目结构图 todo

	数据流转

	组件相关问题
	

	直播大屏
		背景目标
			卖家？需要在dashboard上查看直播的相关数据指标，包括GMV、like、item sold之类，类似与抖音直播看板
		特点
			能够承载高吞吐量5w wps的直播数据，拥有kv、mysql、es、hbase多数据源存储方案，系统高可用，能够进行亚秒级的近实时直播商品数据的查询，为创作者提供了丰富的多口径多维度的直播以及商品数据展示
		难点
			直播数据按地区拆库之后，id地区仍有大量bin log,2天时间1.2t 原因：单场直播的总览数据和分钟数据合并存储，分钟数据格式为map，每次产生一条新的分钟数据时，会对总览数据的某个map中的数据进行累加，bin log为statement，所以占用空间非常大
				解决方案：分地区拆库，合并分钟数据，压缩格式 Kryo序列化（效率高，protobuf不支持原生的java对象序列化）
				数据结构是一个session跟多分钟的map数据吗？对的
			数据更新峰值高，实时和分钟级别数据高峰期qps 3w左右 ，持续时间8分钟，假设数据库upsert每秒5K，那么需要48分钟才能消费完，显然不可接受
				分流，主要指标数据和修正数据分流，主要峰值来源于同一同步的修正数据
				为什么现在qps只有上千？流量被切走了
			数据延迟在大促期间只能保证2min内，主播开播推流后，实时大屏入口迟迟看不见dashboard
				怎么解决？直接监听主播的开播信息
			session更新tps瓶颈，数据都放在一行，导致多线程更新表锁竞争压力大
				拆表0-100，拆分钟数据

		技术挑战
		高吞吐 每日6-8亿条宽表数据，直播大屏峰值5w wps
		低延迟 亚小时搬运亿级别数据，直播实时插入查询亚秒级别
		海量数据 千亿级别卖家数据，es集群商品数据700亿文档，6t存储

		原因：数据维度多 主播维度 主播*商品 主播*直播*商品 时间跨度大 数据要求实时性高，写入场景多，update较多，读取场景稍微少
				演化：
					cronjob 读hdfs 每日百万级数据
						异常数据不兼容
						吞吐量低
						不支持断点续传
					cronjob 调用PrestoSQL 每日亿级数据
						支持数据结构化
						大表支持差
					hive + spark + catalog + kafka + consumer
						支持dag形式的任务编排
						支持断点续传
						sql编写灵活，支持复杂查询
						吞吐量高，kafka容易横向拓展
							db负载高
								做不了读写分离，主读主写
								批插批写不太有效，因为主要是update
								限速削峰 不符合业务场景
								分库分表 已经分了
								tidb 需要付费
							bufferpool 脏页?
							mysql不适合大规模数据
							不适合写多读少场景
							表结构不易变更

			替代方案：kv为辅，hbase为主
			hbase lsm?
			hbase使用场景

		解决方案
		线上问题

		后续优化

		问题：
			大屏为什么不做移动端？ 有的
			kv更新指标如何解决并发问题？	
			kol流量控制如何实现的自动增长？
			hbase存什么
			es存什么
			kv存什么
			mysql存什么

	线上问题
		https://confluence.shopee.io/pages/viewpage.action?pageId=1933551179

	jdk升级
		https://confluence.shopee.io/pages/viewpage.action?pageId=2332565944
		痛点
		必要性
		难点
		解决
		效果

	数据指标
		db占用
		es占用
		kafka

	性能优化
		https://confluence.shopee.io/pages/viewpage.action?pageId=2284141477
		直播大屏es：https://confluence.shopee.io/pages/viewpage.action?pageId=2153729220




	有道少儿英语订单系统拆分


项目介绍

这个订单异步化改造主要是对我们订单原有基于线程池的方案就行了一些优化，因为之前我们订单下单后的一些逻辑，包括开课、短信、活动、物流等，都是在用户支付完成后，在线程池中进行的。

这样就可能会导致一些问题：

首先是当订单量如果突增的话，大量订单会导致线程池拒绝的风险，
二是之前的过程中，对于业务执行失败的订单，缺少了异常记录的操作，
三是所有不同类型的业务耦合性太强，希望去对他进行一个拆分，比如说一笔订单支付成功后，活动的执行失败不应该影响后续开课的业务执行。

因此对他进行了优化，当时采用了canal+kafka的方式，我们这边订单支付后的回调接口，是将订单状态由0改为1，所以canal只需要监听订单表的变化订单；发送到kafka中，对于这个topic我创建了一个负责清洗的消费者，会解析canal发送的消息，只过滤状态值从0变为1的订单数据；

之后将订单id再次进行转发；

另外我是将订单后置的履约业务拆分为独立的互不影响的几个子业务，将一些共有的方法抽象为一个抽象类，其中提供了模板方法，每个子业务只需要继承这个抽象类，在其中定义自己的业务码，编写各自的业务逻辑就可以很好的扩展，也方便了以后其他同事的开发。

每个子业务都作为一个消费者组来对支付成功的订单topic进行监听，接受到以后通过orderId就可以还原订单、商品、用户的一些数据信息，通过在redis分布式锁+mongo中记录执行结果，来确保原子性避免重复消费（因为对每个子业务定义了业务码，可以区分）。其次对于执行异常的订单，会根据订单id、业务码记录下来，通过定时任务补偿的方式确保最终一致性。


可能后续会问道或者可以在介绍中补充的问题：

线程池相关，各个参数和提交流程
canal的机制，mysql数据同步
一致性方案
分布式锁
订单量特别大怎么办：读写分离、缓存、分库分表（https://blog.csdn.net/weixin_48182198/article/details/108475822、）