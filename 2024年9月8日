2024年9月8日
ES:
	question list:
		为什么es可以支持同个索引中的多个字段检索？
		es的前缀树+position dictionary的检索方式和mysql的文本检索方式有什么不同？
		直播大屏 es存几百亿数据，上t的数据量，那es的集群组成是什么样的，用什么分词器，刷盘时间是多少？

		ES是基于倒排索引，那根据某些字段排序是如何实现的？其中数字排序和范围筛选如何实现？
	tip list:
		深度分页问题
		全文检索原理
			前缀树+

	priority list:
		ES问题排查和定位



MySQL:
	既然mysql是先写内存再同步bin log，而redis也是进行内存操作再持久化，为什么redis就是比mysql快呢？

	更新频繁刷新buffer pool?

	两阶段提交？

	priority list:
		不同语句加锁情况
		组合索引案例
		慢查询问题定位和优化案例
		索引失效的情况


	


HBase：
	存储格式和MySQL有什么区别？为什么能用于大数据存储？


	Hbase的rowKey为什么设计成 余数:主播id 的格式？
		1.余数是为了Hbase的数据能够均匀分布在不同region上，不会造成数据倾斜
		2.数据都是以主播为维度，Hbase支持前缀匹配，该格式的rowKey便于scan操作


Kafka:
	question list：
		商品数据的kafka的broker以及parttion是如何分布的，线程池如何设计？
		kafka的消费者数量跟线程数有关还是只跟容器数有关？


Java:

	question list:
		多线程
			创建线程的方式（代码）
				new Thread
				new Thread(Runnable)
				new Thread(new FutureTask(Callable))
			阻塞队列的定义以及不同种类队列的适用范围
				array 有界；积压可控
				link 无界；不了解消息长度
				synchronous 低延迟；无积压

			线程池的参数设置规则；在业务场景中如何设置
				cpu coret: core + 1 maxt: core + 1
				io coret: core * 2 maxt: coret * 2 ~ 4 
			synchronized时，线程是如何阻塞，又是如何被唤醒的
				这都是JVM管理的

			volatile的有序性是如何实现的
				通过读写屏障实现的
			semaphore应用场景

		JVM
			JMM
				公共内存
					堆
						运行时常量池：静态变量
						字符串常量池
					方法区/元空间
						类定义
				私有内存
					虚拟机栈
					本地方法栈
					程序计数器
			垃圾回收器种类
				serial
				parallel
				CMS
					回收流程
					为什么处理不了浮动垃圾？
				G1
					特点
						分区
						停顿可预测
					参数设置
					回收流程
					缺陷
						主要的STW为复制-转移过程中，时间与对象的大小和复杂程度成正比，极端情况可能造成若干秒的STW
						其中转移过程不能并发根本原因为无法对对象准确寻址
				ZGC
					特点
						读屏障：对对象进行操作时，会更新对象的新地址，确保始终访问的都是新地址
						着色指针：通过地址记录对象的存活信息，无需寻址查看对象头
					参数设置
						con_gc_threads
						par_gc_threads	

			垃圾回收算法
				复制
				标记清除/整理
				分代
					分代大小比例
					分配担保
			类加载机制
				类加载器
					bootstrap
					ext
					app
					user define
				双亲委派


项目经历 2h

内容供给团队
	主要为创作者服务，帮助创作者生产优质内容，使其能够获得更好的涨粉、创作收益，进而实现更好的带货变现

业务功能
	创作中心
		数据分析
		内容变现
		创作工具
		自动化运营&激励功能
	运营平台
		流量包管理
		活动&课程管理

业务系统
	creator-centor 创作者入口
		创作者数据处理 core data bin log
		作者推荐服务：给用户推荐作者，帮助作者涨粉
		榜单服务：给用户展示视频排名、作者排名榜单
		任务系统：让创作者完成一系列任务，提高创作者积极性
		课程服务：给创作者提供课程服务，增加专业知识
		带货数据分析：给作者提供视频带货订单分析
		权益流量包：给作者发放流量包，失活作者召回
	creator-market 撮合平台，创作者洽谈合作的平台
		邀约系统：同谷歌日历
		创作者信息维护
		通知提醒服务
	admin-platform
		流量扶持：发布活动，给某些大v的视频增加流量
		保量数据分析：数仓数据聚合筛选同步

基础服务
	spex4j rpc服务
	config-server 配置中心
	data-park 数据特征标签服务
	creator-payment 支付服务

参考：https://confluence.shopee.io/pages/viewpage.action?pageId=1220060040


工作职责
	video supply基础数据中心服务
	涉及技术：redis、kafka、xxl-job、MySQL、rocksDB、ES
	通过搭建统一的标签&特征服务，提供高性能的人群\视频\商品判定和特征查询服务，支撑上层业务需求迭代和快速落地
	成果：通过搭建特征标签平台，减少了特征数据对接的流程，将涉及到数据相关的需求的平均开发时长缩短了20%

	业务挑战
			高吞吐
				每日300+ 数据流转job
				每日 6-8亿条离线宽表数据
				实时数据 大屏峰值5w+ wps 整体msg in 10w+

				实现：通过spark 监听hive表的marker，将catlog传输到kafka，通过kafka多partition横向拓展
				问题：吞吐量高造成db负载压力过高 10w的ops，读写比2:1 基于业务需要主库读主库写，不能做读写分离
						合并写入：直播的归单逻辑要求有查询有更新
						限速削峰：必须马上落盘，实时性要求不能写入
						TiDB：不能支持这么大规模数据，更大规模需要付费
						分库分表：不适合管理大规模数据，顶配的机器写入2w
				解决：HBase为主 8T存储，KV为辅助
			低延迟
				亚小时搬运亿级别数据 > 28k wps
				直播实时指标亚秒级查询

				解决：写入kv

			海量数据
				es 700e文档
				seller center千亿文档

				解决：选择合适的存储 KV实时数据 ES多维度查询数据 HBase离线数据 
		架构演进
			1.xxl job读hdfs
			2.cron job读hdfs的data service api
			3.hive + spark catalog + kafka + consumer
				极高的吞吐
				支持断点续传
				支持多样的SQL
		数据指标
			ID/VN 16台46G RDS， 总计10w OPS， 读写比2：1
			es 20亿直播间数据 700亿文档 6t存储 15k rps 12台
		方案选型
			mysql 因为更新特别多，会触发buffer pool频繁更新写脏页
			hbase 因为主要需要聚合 8t存储 峰值 2g磁盘io 1g网络io
				合理设计rowkey
				start stop减少扫描行数
				filter过滤数据

	特征平台

		数据源 -> 数据 -> 整合处理 -> kv(用作特征匹配判定根据userId查相应特征 userId - 多特征key -> value)    通过特征圈选人群包、传入userId判定特征
						-> es（多特征到userId的映射） 



		用户特征  来源 应用场景



		资源申请
			copi2：	key个数 7000w
					key类型 hash 
					key长度 20byte
					字段数 200
					字段长度 10byte
					value长度 10byte
					总计：500G

			运行模式：高性能/大存储
			预估总容量（key+value+buffer）1000G
			预估QPS：20000 = 1000（下游服务qps） * 8（下游服务个数） read 120（特征接入任务数） * 100（单个任务qps） write
			使用命令：hget hgetall hset
			读/写命令分布占比：40% 60%
			最大value长度 200 * 3byte
			平均value长度 10byte
			key总数量 7000w
			平均key长度 10byte
			平均TTL -1
			客户端并发连接数 100
			预期读命令P99 10ms
			预期写命令P99 10ms
			是否有plan B 有
			能否容忍机房故障 不能


			es: 单文档大小 =  200 * 4byte(普通数字) + 20 * 50字符 * 3byte（utf8字符串） = 3800byte / 3.8kB
				索引数 = 2
				文档总数 = 2000w(用户特征) + 5000w(视频特征)
				源数据大小 = 3.8kB * 7000w = 266GB 
				总容量大小 = 源数据大小 * (1 + 副本数量) * (1 + 索引开销) / （1 - 操作系统预留）/ (1 - 内部任务开销) * (1 + 预留空间) 
						 = 266GB * (1 + 1) * (1 + 0.1) / (1 - 0.05) / (1 - 0.2) * (1 + 0.5)
						 = 266GB * 4.35
						 = 1.2TB


			版本号：7.8.1
			业务场景描述：存用户/视频特征数据
			预估容量（总容量，包含副本）： 1.2TB （600G + 副本）
			副本数：1
			索引数量：2（用户表 + 视频表）
			文档数量：7000W = 2000w(用户总数) + 5000w(视频总数)
			index rate（写入速率，docs/s，index rate=写入文档数*副本数）：100（单个任务docs/s） * 100(任务数) * 1（副本数） = 10000
			单文档平均大小（kb）：3.8kb
			search rate（查询速率，search rate=查询qps*单次qps所查询所有索引的主分片之和，自定义路由除外）：100（qps） * 200（分片数） = 20000
			查询类型（term match，agg....等等）：term match
			整个集群总的分片数：2索引*100分片每个索引=200
			PS：1.es索引设计注意点：1.单分片容量尽量不要超过20G，2.单分片doc条数硬上限20亿条
			        2.各种复杂的分词逻辑尽量不要用es去完成，比较损耗es性能，尤其是ngram分词器或者一些第三方不可控的分词器。


	直播大屏
		es/kv和db的数据一致性如何保证
			db作为兜底方案并没有强制要求一致性，只要主链路（写kv/es）没问题就行
		不同数据源对应的业务场景，为何这么选型
			直播大屏
				session item分钟级/总览数据 ES + db 需要商品名称模糊检索 需要按滑动窗口存储分钟级数据，而es能够支持筛选数组中的元素，类型为nested
				session 总览数据 kv + db 需要快速检索
				session 分钟级数据 kv + db kv的《hash》中的结构为views: {1111: xxx, 1112: xxx} key为时间戳，	kv设置为7天过期 不到10g
			内部看板
				session list 数据 ES  需要按照多个字段排序以及检索 和session总览数据一样，只是不一样的链路 sessionOverview到底存哪了？kv + es + db
				session rank kv zset 每个维度一个key，按照权值进行排序


		不同数据源的数据量
			Topic 1	ls_mart.ads_session_live_insight{region} 	3k （ID）	直播间总揽数据	DB +ToC Codis + ToB Codis  + HBASE
				RealTimeSessionOverviewConsumer 主链路写 KV + 旁路写DB热备   && LsSessionLeaderboardConsumer 更新某个地区某个维度的榜单，比如ID的GMV， order， item sold榜单，用zset  
			Topic 2	ls_mart.ads_session_live_insight_mi{region}	3k（ID）	直播间分钟数据	DB +ToC Codis + ToB Codis + HBASE
				RealTimeSessionMinuteConsumer 主链路写 KV 十分钟一条数据（和mysql类似对应字段hash） + 旁路写DB热备   && LsSessionLeaderboardConsumer 更新某个地区CCU（在线用户数）的榜单，因为要求实时数据，用zset  
				LsInternalSessionMinuteDataConsumer 写直播开播后总观看数的（为什么不用overview数据？）
			Topic 3	ls_mart.ads_session_item_live_insight{region} 	2w/s（ID）	商品总揽数据	ToC ES + DB(没看到写DB) + 	HBASE
				RealTimeItemOverviewConsumer 主链路写ES，用于直播间的商品列表  + LsItemLeaderboardConsumer 写kv 用于直播间的商品排行榜 + 写kv 用于记录某个地区某件商品的历史销量 + 直播下商品的overview kv LsItemAccumulateDataConsumer
			Topic 4	ls_mart.ads_session_item_live_insight_mi{region}	2w/s（ID）	商品分钟数据	ToC ES + DB + ? （应该没有写HBASE，因为这种实时数据没必要？）
				主链路写ES(RealTimeItemMinuteBatchConsumer) 用于更新ES中分钟级窗口（部分指标是5分钟窗口，部分指标是全部窗口） RealTimeItemMinuteConsumer写DB，十分钟一条数据，各维度指标用Kryo序列化（理由：只有java内部使用场景，和pb以及thrift序列化性能极高，反正都是不可读，选个效率高的）
				数据一致性（假设有校对job）

			Update link1	限速2w/s以内	DB热备 topic 1-4的数据，限速更新，KOL的数据单独消费队列不限速
			Update link2	>10w/s	直播间多指标展开存储更新，有写扩散，整体控制在 topic 1+2的 10倍
			Update link3	30w/s	topic 1 + 2 + 3的数据，做延迟打点监控，内部dashboard看板，写扩散多
			Update link4	4W/s	topic 3+4的数据update

		Hbase存什么
			离线数据
				streamer * item 10T 每月0.8T ? 为什么数据这么多
				streamer * session * munite  2T 每月0.16T 
				streamer * session * item < 1T

		kv
			直播商品分钟级数据（榜单+分钟级） 6g 2000w key qps 2w 跟kafka能对上
			直播分钟级数据 6g 800w key qps 3k

		

		线上问题
		你负责什么
			拆库拆表
			持久化逻辑
			后续持续优化
		遇到什么问题
			榜单消失 多线程对zset add del
			kafka poll time超时


		如何解决
		成效如何
		优化迭代



分布式 2h
	spex的io模型

	服务调用时有什么降级熔断策略
		降级有，普通请求打error，不影响主链路，关键请求有限流开关，保证大v和头部主播能正常看到数据
		熔断，有些写db且实时性要求不高的请求会加熔断器，有几个作用：快速失败、保护系统不接受额外的请求降低压力

	leaf会有时钟回拨问题吗

	lsm tree原理
		写内存 + 顺序写磁盘 代替 随机写磁盘
		https://wingsxdu.com/posts/database/leveldb/#%E6%80%BB%E7%BB%93

	系统在 高性能 高可用 高拓展 方面有什么优化
		高性能 kv es hbase等存储用于应对不同的业务场景 通过kafka进行消费
		高可用 服务本身有降级熔断 降低采样率 等策略 不同的中间件也是集群分片部署
		高拓展 代码拓展性高，微服务组件支持快速搭建部署不同功能服务

Redis 3h
	线程模型
		单线程
		io多路复用
			大量并发请求时如何高效处理请求？
				epoll和select的对比
		epoll
	分布式
		主从
			备份方式
				rdb
				aof
			效率对比以及redis的内存大小设置？
				对主线程影响 rdb < aof
				文件体积 rdb < aof
				可靠性 rdb < aof
				设置为最大内存的1/2，极端情况内存所有数据都被更改时，子进程用于rdb备份的内存数据和父进程被更改的数据各占一半

		集群
			一致性哈希

			故障转移

			哨兵
				所以总结来说cluster是通过多个节点相互通信确认节点健康状态，而sentinel是通过选举sentinel领头节点去负责已知的主节点故障转移
		应用场景
			问题
				缓存击穿 热点key过期
				缓存雪崩 大量key过期
				缓存穿透 非法的数据请求
					布隆过滤器
			db缓存策略
				先db 后缓存



设计模式 2h


中间件版本、配置、架构、数据量、性能、可用性 1h

业务场景
	分布式id
		leaf如何解决分布式id的workid和data center id?


难点
	系统可优化的话，有什么拓展性
	leaf源码
	







踩坑记录：
	https://docs.google.com/document/d/1OPU85uOrNKOGEb7Y4Q05h4hVkyHW7qLBIsFpWcBZNT0/edit#heading=h.9gp5ayiy89h

数据整理
线上慢查询
系统设计题
lc

线上问题：
	mysql慢查询优化
		in列表太多
		未命中索引

	redis：
		通过Prometheus埋点监控热点Key，通过Prometheus查询接口精准过滤查询并定时更新进入本地内存，减轻redis负担，相比于全量缓存本地更加节省内存。


	es:
		性能优化
			异步落盘 5s异步刷translog
			批插批写
			模型优化
				只需要聚合不需要搜索，index设成false
				不需要算分，Norms设成false
				禁止使用dynamic mapping，避免字段数量过多
				优化index_options，控制在创建索引时哪些字段会添加到倒排索引
				根据业务需求，选择是否关闭_source
			路由
				使用ES默认路由: 按文档id路由, 文档id : ${sessionId}-${itemId}
				确保ES分片均匀的分布在各个物理节点上
				设置合理的分片数
			cpu和io
				减少不必须的分词
				避免不需要的doc_values
				提高文档压缩率-文档写入时保证字段顺序
				提高文档压缩率-当字段值不变或字段为0时可以不写入 
				ES配置- refresh interval（牺牲搜索实时性）
				ES配置- translog async（牺牲可靠性）
				根据业务需求, 选择是否使用ES自动生成的文档id
	kafka:
		一次性拉取过多，导致消费速度过慢超时，超过max.poll.interval.ms发生rebalance，重新跟broker建立链接之后，又从原先的offset开始消费
		相同broker，不同topic，group id设置相同,但是topic鉴权不相同，导致rebalance之后无法消费，消息堆积



最近看过什么书/源码
为什么想换工作，对下一份工作有什么期望
团队的业务前景：虽然有高并发大流量的系统，但是业务环节相对薄弱，希望能够接触更复杂的业务，从而实现自我提升
晋升和薪资方面：希望承担更多的工作量和挑战，使自己的薪资上涨
公司的发展前景：tt和拼多多在东南亚发力，挤占了shopee的市场份额，对公司未来前景有顾虑
我希望自己在团队中能担当关键的技术角色，尤其是优化现有系统，推动其在高并发场景中的稳定性和效率。通过这样的历练，也相信能为公司和团队创造更大的价值
你如何评价自己，什么优点、什么缺点
优点：善于解决问题和跨团队沟通，能够尽可能地使方案落地，并且持续技术输出，保证团队产能
缺点：异常业务复杂场景可能还需要实际地接触和了解，
在项目中遇到的最难的点是什么，如何解决的
在直播大屏系统从0到1的演进过程中，遇到的包括海量数据、高吞吐、低延迟的问题，在确认好通用的解决方案后，通过xxxx手段去适配调优，并且在上线之后有大大小小的线上问题，然后不断地维护迭代升级系统，这是我觉得最难的点，尤其是在shopee这种基建不那么成熟的公司里
这几年最大的收获/成长是什么
从最开始的闷头做业务需求，到后来的逐渐去关注团队的一些系统架构和设计，然后不断地去参与其中的项目搭建和优化，再到后来独立owner一个项目，从初期技术选型到方案评审再到风险评估，以及上线后的系统高可用稳定性的维护，不断地精进技术以及提升自己在团队/公司中的影响力，从而去给整个团队提效，帮助自身和同事提升自身的产出，在技术水平、团队影响力、跨团队沟通方面有了提升/成长
换工作的逻辑是什么，就职不同的公司有什么选择逻辑
探迹：初创公司，为了提升自身的技术广度
有道：大厂，为了了解规范的业务，以及提升技术的深度，应用自身的经验去落地项目
shopee：大厂，为了提升自己在面对大流量高并发场景的开发经验，以及对跨区域国际化项目的了解程度
你主要负责项目中的哪一部分，你对自己在团队中的角色定位是什么
主要负责日常业务逻辑的开发，以及包括mysql拆分，es的优化，以及上线之后一些流量报警的处理，并在此基础上对存储和架构进行进一步优化
给自己的定位：业务开发&技术输出者&救火队员



订单系统负责哪个模块 订单创建 库存管理 支付模块 订单处理（状态流转）物流管理 用户管理 促销优惠 订单通知 售后管理 报表监控  后续的履约流程
为什么订单不直接发消息 解耦、可靠性、一致性保障（防止出现发消息不成功但是db修改成功的情况）、容易审计（其他消费者也可通过监听bin log统计订单的状态变化和流转）
假设同时监听订单和库存bin log不一致怎么办 加事务 加分布式锁 扣库存的方式 预扣库存 下单后扣库存 支付后扣库存 
订单支付失败后怎么办 支付宝/微信有失败重试机制 可以主动查询支付状态
订单能不能回退 可以取消 但是支付完成不支持回退
消息丢了怎么办 核对订单状态，定时任务检查订单状态 自动补偿机制 人工干预
ThreadLocal丢失数据 gc会把key的弱引用回收，get的时候就get不到值了 https://cloud.tencent.com/developer/article/1563309
spring事务隔离级别 加入事务 新创建事务 不用事务 


分库分表怎么分，为什么这么分，为什么需要分表，为什么是16个库100张表 统计的数据量是保证 
有哪些数据维度
	直播总览 3k
	直播的分钟级趋势数据 3k
	商品的总览数据 2w
	商品的分钟级趋势数据 2w
	直播间基础信息 实时rpc
	违规记录 个位数


怎么评价自己，有什么优势，顺着团队工作的偏好说，喜欢稳定

有没有其他offer 有一个xx公司的，方向差不多，但是在考虑

为什么选择我们 顺着团队公司说

订单系统 1 √ 
spring 2 √
按题检索 3 √
interview summary 4 5 6 
面试总结 7
leetcode every day
秒杀系统


coupang

1面
发券
1.后台发券
2.用户自行领券

券类型
1.平台通用券
2.店铺券

细分
1.立减券
2.满减券	


数据库设计
系统架构图
系统的api



10w用户 1w订单 单表存储 1年预估1000w数据量

优惠券类型表 platform 平台/店铺 type 立减/满减 amount 优惠金额
用户表 uid
店铺表 shop id
优惠券表 coupou id  platform type amount (shop id)
优惠券库存表 coupou id count
用户优惠券表 user coupou id uid coupou id
订单表 order id
订单用券数据表（每笔订单用的券 order id user coupou id

2面
文件系统

创建文件/文件夹
删除文件/文件夹
打印文件夹下所有文件/文件夹
编辑文件
打印文件

3面
内存型的微博

关注
取关
发布微博
获取关注列表最近10条微博


hbase存储的数据格式
如何保障代码
分partition的消息顺序性如何保证
重大线上问题处理
