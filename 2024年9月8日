2024年9月8日
ES:
	question list:
		为什么es可以支持同个索引中的多个字段检索？
		es的前缀树+position dictionary的检索方式和mysql的文本检索方式有什么不同？
		直播大屏 es存几百亿数据，上t的数据量，那es的集群组成是什么样的，用什么分词器，刷盘时间是多少？

		ES是基于倒排索引，那根据某些字段排序是如何实现的？其中数字排序和范围筛选如何实现？
	tip list:
		深度分页问题
		全文检索原理
			前缀树+

	priority list:
		ES问题排查和定位



MySQL:
	既然mysql是先写内存再同步bin log，而redis也是进行内存操作再持久化，为什么redis就是比mysql快呢？

	更新频繁刷新buffer pool?

	两阶段提交？

	priority list:
		不同语句加锁情况
		组合索引案例
		慢查询问题定位和优化案例
		索引失效的情况


	


HBase：
	存储格式和MySQL有什么区别？为什么能用于大数据存储？


	Hbase的rowKey为什么设计成 余数:主播id 的格式？
		1.余数是为了Hbase的数据能够均匀分布在不同region上，不会造成数据倾斜
		2.数据都是以主播为维度，Hbase支持前缀匹配，该格式的rowKey便于scan操作


Kafka:
	question list：
		商品数据的kafka的broker以及parttion是如何分布的，线程池如何设计？
		kafka的消费者数量跟线程数有关还是只跟容器数有关？


Java:

	question list:
		多线程
			创建线程的方式（代码）
				new Thread
				new Thread(Runnable)
				new Thread(new FutureTask(Callable))
			阻塞队列的定义以及不同种类队列的适用范围
				array 有界；积压可控
				link 无界；不了解消息长度
				synchronous 低延迟；无积压

			线程池的参数设置规则；在业务场景中如何设置
				cpu coret: core + 1 maxt: core + 1
				io coret: core * 2 maxt: coret * 2 ~ 4 
			synchronized时，线程是如何阻塞，又是如何被唤醒的
				这都是JVM管理的

			volatile的有序性是如何实现的
				通过读写屏障实现的
			semaphore应用场景

		JVM
			JMM
				公共内存
					堆
						运行时常量池：静态变量
						字符串常量池
					方法区/元空间
						类定义
				私有内存
					虚拟机栈
					本地方法栈
					程序计数器
			垃圾回收器种类
				serial
				parallel
				CMS
					回收流程
					为什么处理不了浮动垃圾？
				G1
					特点
						分区
						停顿可预测
					参数设置
					回收流程
					缺陷
						主要的STW为复制-转移过程中，时间与对象的大小和复杂程度成正比，极端情况可能造成若干秒的STW
						其中转移过程不能并发根本原因为无法对对象准确寻址
				ZGC
					特点
						读屏障：对对象进行操作时，会更新对象的新地址，确保始终访问的都是新地址
						着色指针：通过地址记录对象的存活信息，无需寻址查看对象头
					参数设置
						con_gc_threads
						par_gc_threads	

			垃圾回收算法
				复制
				标记清除/整理
				分代
					分代大小比例
					分配担保
			类加载机制
				类加载器
					bootstrap
					ext
					app
					user define
				双亲委派


项目经历 2h

内容供给团队
	主要为创作者服务，帮助创作者生产优质内容，使其能够获得更好的涨粉、创作收益，进而实现更好的带货变现

业务功能
	创作中心
		数据分析
		内容变现
		创作工具
		自动化运营&激励功能
	运营平台
		流量包管理
		活动&课程管理

业务系统
	creator-centor 创作者入口
		创作者数据处理 core data bin log
		作者推荐服务：给用户推荐作者，帮助作者涨粉
		榜单服务：给用户展示视频排名、作者排名榜单
		任务系统：让创作者完成一系列任务，提高创作者积极性
		课程服务：给创作者提供课程服务，增加专业知识
		带货数据分析：给作者提供视频带货订单分析
		权益流量包：给作者发放流量包，失活作者召回
	creator-market 撮合平台，创作者洽谈合作的平台
		邀约系统：同谷歌日历
		创作者信息维护
		通知提醒服务
	admin-platform
		流量扶持：发布活动，给某些大v的视频增加流量
		保量数据分析：数仓数据聚合筛选同步

基础服务
	spex4j rpc服务
	config-server 配置中心
	data-park 数据特征标签服务
	creator-payment 支付服务

参考：https://confluence.shopee.io/pages/viewpage.action?pageId=1220060040


工作职责
	video supply基础数据中心服务
	涉及技术：redis、kafka、xxl-job、MySQL、rocksDB、ES
	通过搭建统一的标签&特征服务，提供高性能的人群\视频\商品判定和特征查询服务，支撑上层业务需求迭代和快速落地
	成果：通过搭建特征标签平台，减少了特征数据对接的流程，将涉及到数据相关的需求的平均开发时长缩短了20%

	业务挑战
			高吞吐
				每日300+ 数据流转job
				每日 6-8亿条离线宽表数据
				实时数据 大屏峰值5w+ wps 整体msg in 10w+
			低延迟
				亚小时搬运亿级别数据 > 28k wps
				直播实时指标亚秒级查询

			海量数据
				es 700e文档
				seller center千亿文档
		架构演进
			1.xxl job读hdfs
			2.cron job读hdfs的data service api
			3.hive + spark catalog + kafka + consumer
				极高的吞吐
				支持断点续传
				支持多样的SQL
		数据指标
			ID/VN 16台46G RDS， 总计10w OPS， 读写比2：1
			es 20亿直播间数据 700亿文档 6t存储 15k rps 12台
		方案选型
			mysql 因为更新特别多，会触发buffer pool频繁更新写脏页
			hbase 因为主要需要聚合 8t存储 峰值 2g磁盘io 1g网络io
				合理设计rowkey
				start stop减少扫描行数
				filter过滤数据

	特征平台

		数据源 -> 数据 -> 整合处理 -> kv(用作特征匹配判定根据userId查相应特征 userId - 多特征key -> value)    通过特征圈选人群包、传入userId判定特征
						-> es（多特征到userId的映射） 



		用户特征  来源 应用场景



		资源申请
			copi2：	key个数 7000w
					key类型 hash 
					key长度 20byte
					字段数 200
					字段长度 10byte
					value长度 10byte
					总计：500G

			运行模式：高性能/大存储
			预估总容量（key+value+buffer）1000G
			预估QPS：20000 = 1000（下游服务qps） * 8（下游服务个数） read 120（特征接入任务数） * 100（单个任务qps） write
			使用命令：hget hgetall hset
			读/写命令分布占比：40% 60%
			最大value长度 200 * 3byte
			平均value长度 10byte
			key总数量 7000w
			平均key长度 10byte
			平均TTL -1
			客户端并发连接数 100
			预期读命令P99 10ms
			预期写命令P99 10ms
			是否有plan B 有
			能否容忍机房故障 不能


			es: 单文档大小 =  200 * 4byte(普通数字) + 20 * 50字符 * 3byte（utf8字符串） = 3800byte / 3.8kB
				索引数 = 2
				文档总数 = 2000w(用户特征) + 5000w(视频特征)
				源数据大小 = 3.8kB * 7000w = 266GB 
				总容量大小 = 源数据大小 * (1 + 副本数量) * (1 + 索引开销) / （1 - 操作系统预留）/ (1 - 内部任务开销) * (1 + 预留空间) 
						 = 266GB * (1 + 1) * (1 + 0.1) / (1 - 0.05) / (1 - 0.2) * (1 + 0.5)
						 = 266GB * 4.35
						 = 1.2TB


			版本号：7.8.1
			业务场景描述：存用户/视频特征数据
			预估容量（总容量，包含副本）： 1.2TB （600G + 副本）
			副本数：1
			索引数量：2（用户表 + 视频表）
			文档数量：7000W = 2000w(用户总数) + 5000w(视频总数)
			index rate（写入速率，docs/s，index rate=写入文档数*副本数）：100（单个任务docs/s） * 100(任务数) * 1（副本数） = 10000
			单文档平均大小（kb）：3.8kb
			search rate（查询速率，search rate=查询qps*单次qps所查询所有索引的主分片之和，自定义路由除外）：100（qps） * 200（分片数） = 20000
			查询类型（term match，agg....等等）：term match
			整个集群总的分片数：2索引*100分片每个索引=200
			PS：1.es索引设计注意点：1.单分片容量尽量不要超过20G，2.单分片doc条数硬上限20亿条
			        2.各种复杂的分词逻辑尽量不要用es去完成，比较损耗es性能，尤其是ngram分词器或者一些第三方不可控的分词器。


	直播大屏
		es/kv和db的数据一致性如何保证
			db作为兜底方案并没有强制要求一致性，只要主链路（写kv/es）没问题就行
		不同数据源对应的业务场景，为何这么选型
			直播大屏
				session item分钟级/总览数据 ES + db 需要商品名称模糊检索 需要按滑动窗口存储分钟级数据，而es能够支持筛选数组中的元素，类型为nested
				session 总览数据 kv + db 需要快速检索
				session 分钟级数据 kv + db kv的key为时间戳，	
			内部看板
				session list 数据 ES  需要按照多个字段排序以及检索 和session总览数据一样，只是不一样的链路 sessionOverview到底存哪了？kv + es + db
				session rank kv zset 每个维度一个key，按照权值进行排序


		不同数据源的数据量
			Topic 1	ls_mart.ads_session_live_insight{region} 	3k （ID）	直播间总揽数据	DB +ToC Codis + ToB Codis
			Topic 2	ls_mart.ads_session_live_insight_mi{region}	3k（ID）	直播间分钟数据	DB +ToC Codis + ToB Codis
			Topic 3	ls_mart.ads_session_item_live_insight{region} 	2w/s（ID）	商品总揽数据	ToC ES + DB
			Topic 4	ls_mart.ads_session_item_live_insight_mi{region}	2w/s（ID）	商品分钟数据	ToC ES + DB

			Update link1	限速2w/s以内	DB热备 topic 1-4的数据，限速更新，KOL的数据单独消费队列不限速
			Update link2	>10w/s	直播间多指标展开存储更新，有写扩散，整体控制在 topic 1+2的 10倍
			Update link3	30w/s	topic 1 + 2 + 3的数据，做延迟打点监控，内部dashboard看板，写扩散多
			Update link4	4W/s	topic 3+4的数据update

		线上问题
		你负责什么
			拆库拆表
			持久化逻辑
			后续持续优化
		遇到什么问题
			榜单消失 多线程对zset add del
			kafka poll time超时


		如何解决
		成效如何
		优化迭代



分布式 2h
	spex的io模型

	服务调用时有什么降级熔断策略
		降级有，普通请求打error，不影响主链路，关键请求有限流开关，保证大v和头部主播能正常看到数据
		熔断，有些写db且实时性要求不高的请求会加熔断器，有几个作用：快速失败、保护系统不接受额外的请求降低压力

	leaf会有时钟回拨问题吗

	lsm tree原理
		写内存 + 顺序写磁盘 代替 随机写磁盘
		https://wingsxdu.com/posts/database/leveldb/#%E6%80%BB%E7%BB%93

	系统在 高性能 高可用 高拓展 方面有什么优化
		高性能 kv es hbase等存储用于应对不同的业务场景 通过kafka进行消费
		高可用 服务本身有降级熔断 降低采样率 等策略 不同的中间件也是集群分片部署
		高拓展 代码拓展性高，微服务组件支持快速搭建部署不同功能服务

Redis 3h
	线程模型
		单线程
		io多路复用
			大量并发请求时如何高效处理请求？
				epoll和select的对比
		epoll
	分布式
		主从
			备份方式
				rdb
				aof
			效率对比以及redis的内存大小设置？
				对主线程影响 rdb < aof
				文件体积 rdb < aof
				可靠性 rdb < aof
				设置为最大内存的1/2，极端情况内存所有数据都被更改时，子进程用于rdb备份的内存数据和父进程被更改的数据各占一半

		集群
			一致性哈希

			故障转移

			哨兵
				所以总结来说cluster是通过多个节点相互通信确认节点健康状态，而sentinel是通过选举sentinel领头节点去负责已知的主节点故障转移
		应用场景
			问题
				缓存击穿 热点key过期
				缓存雪崩 大量key过期
				缓存穿透 非法的数据请求
					布隆过滤器
			db缓存策略
				先db 后缓存



设计模式 2h


中间件版本、配置、架构、数据量、性能、可用性 1h

业务场景
	分布式id
		leaf如何解决分布式id的workid和data center id?







踩坑记录：
	https://docs.google.com/document/d/1OPU85uOrNKOGEb7Y4Q05h4hVkyHW7qLBIsFpWcBZNT0/edit#heading=h.9gp5ayiy89h


为什么想换工作
最近看过什么书/源码


订单系统负责哪个模块 订单创建 库存管理 支付模块 订单处理（状态流转）物流管理 用户管理 促销优惠 订单通知 售后管理 报表监控  后续的履约流程
为什么订单不直接发消息 解耦、可靠性、一致性保障（防止出现发消息不成功但是db修改成功的情况）、容易审计（其他消费者也可通过监听bin log统计订单的状态变化和流转）
假设同时监听订单和库存bin log不一致怎么办 加事务 加分布式锁 扣库存的方式 预扣库存 下单后扣库存 支付后扣库存 
订单支付失败后怎么办 支付宝/微信有失败重试机制 可以主动查询支付状态
订单能不能回退 可以取消 但是支付完成不支持回退
消息丢了怎么办 核对订单状态，定时任务检查订单状态 自动补偿机制 人工干预
ThreadLocal丢失数据 gc会把key的弱引用回收，get的时候就get不到值了 https://cloud.tencent.com/developer/article/1563309
spring事务隔离级别 加入事务 新创建事务 不用事务 

订单系统 1 √ 
spring 2 √
按题检索 3
interview summary 4 5 6
面试总结 7
leetcode every day
秒杀系统
